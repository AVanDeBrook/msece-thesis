
@article{allen_unicode_2012,
  title   = {The unicode standard},
  journal = {Mountain view, CA},
  author  = {Allen, Julie D and Anderson, Deborah and Becker, Joe and Cook, Richard and Davis, Mark and Edberg, Peter and Everson, Michael and Freytag, Asmus and Iancu, Laurentiu and Ishida, Richard},
  year    = {2012},
  note    = {Publisher: Citeseer},
  pages   = {660--664}
}

@inproceedings{arnold_knowledge_2022,
  address   = {Hybrid: Seattle, Washington + Online},
  title     = {Knowledge extraction from aeronautical messages ({NOTAMs}) with self-supervised language models for aircraft pilots},
  url       = {https://aclanthology.org/2022.naacl-industry.22},
  doi       = {10.18653/v1/2022.naacl-industry.22},
  abstract  = {During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This self-supervised approach, where smaller amounts of labeled data are enough for task-specific fine-tuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models (by pilots, airlines or service providers), which is a first to the best of our knowledge.},
  urldate   = {2022-08-25},
  booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Track}},
  publisher = {Association for Computational Linguistics},
  author    = {Arnold, Alexandre and Ernez, Fares and Kobus, Catherine and Martin, Marion-Cécile},
  month     = jul,
  year      = {2022},
  pages     = {188--196}
}


@article{badrinath_automatic_2022,
  title    = {Automatic {Speech} {Recognition} for {Air} {Traffic} {Control} {Communications}},
  volume   = {2676},
  issn     = {0361-1981},
  url      = {https://doi.org/10.1177/03611981211036359},
  doi      = {10.1177/03611981211036359},
  abstract = {A significant fraction of communications between air traffic controllers and pilots is through speech, via radio channels. Automatic transcription of air traffic control (ATC) communications has the potential to improve system safety, operational performance, and conformance monitoring, and to enhance air traffic controller training. We present an automatic speech recognition model tailored to the ATC domain that can transcribe ATC voice to text. The transcribed text is used to extract operational information such as call-sign and runway number. The models are based on recent improvements in machine learning techniques for speech recognition and natural language processing. We evaluate the performance of the model on diverse datasets.},
  language = {en},
  number   = {1},
  urldate  = {2022-07-13},
  journal  = {Transportation Research Record},
  author   = {Badrinath, Sandeep and Balakrishnan, Hamsa},
  month    = jan,
  year     = {2022},
  note     = {Publisher: SAGE Publications Inc},
  keywords = {Read},
  pages    = {798--810},
  file     = {SAGE PDF Full Text:/home/avandebrook/Zotero/storage/N8UWGZQC/Badrinath and Balakrishnan - 2022 - Automatic Speech Recognition for Air Traffic Contr.pdf:application/pdf}
}

@inproceedings{breunig_lof_2000,
  address    = {Dallas Texas USA},
  title      = {{LOF}: identifying density-based local outliers},
  isbn       = {978-1-58113-217-5},
  shorttitle = {{LOF}},
  url        = {https://dl.acm.org/doi/10.1145/342009.335388},
  doi        = {10.1145/342009.335388},
  language   = {en},
  urldate    = {2023-09-13},
  booktitle  = {Proceedings of the 2000 {ACM} {SIGMOD} international conference on {Management} of data},
  publisher  = {ACM},
  author     = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, Jörg},
  month      = may,
  year       = {2000},
  pages      = {93--104}
}

@article{burkardt_truncated_2014,
  title   = {The truncated normal distribution},
  volume  = {1},
  journal = {Department of Scientific Computing Website, Florida State University},
  author  = {Burkardt, John},
  year    = {2014},
  pages   = {35}
}

@inproceedings{cordero_automated_2012,
  title  = {Automated speech recognition in {ATC} environment},
  author = {Cordero, José Manuel and Dorado, Manuel and de Pablo, José Miguel},
  year   = {2012},
  pages  = {46--53},
  file   = {Cordero et al. - 2012 - Automated speech recognition in ATC environment.pdf:/home/avandebrook/Zotero/storage/Y533SDEZ/Cordero et al. - 2012 - Automated speech recognition in ATC environment.pdf:application/pdf}
}

@techreport{cotton_development_1983,
  title    = {Development of {Speech} {Input}/{Output} {Interfaces} for {Tactical} {Aircraft}},
  url      = {https://apps.dtic.mil/sti/citations/ADA136485},
  abstract = {This report covers the methods and the results in the selection of tasks for speech inputoutput IO in the fighter cockpit of the future. A description is given of the candidate speech tasks derived from interviews with experienced F-16 pilots, an analysis of a composite air-to-ground scenario, and a questionnaire survey of F-16 pilots at Nellis AFB.},
  language = {en},
  urldate  = {2023-07-14},
  author   = {Cotton, John C. and McCauley, Michael E. and North, Roberta A. and Strieb, Melvin I.},
  month    = jul,
  year     = {1983},
  note     = {Section: Technical Reports},
  pages    = {185},
  file     = {1983 - Development of Speech InputOutput Interfaces for .pdf:/home/avandebrook/Zotero/storage/6HZLVZ56/1983 - Development of Speech InputOutput Interfaces for .pdf:application/pdf;Snapshot:/home/avandebrook/Zotero/storage/4H9J6P8N/ADA136485.html:text/html}
}

@inproceedings{delpech_real-life_2018,
  title    = {A {Real}-life, {French}-accented {Corpus} of {Air} {Traffic} {Control} {Communications}},
  url      = {https://hal.science/hal-01725882},
  abstract = {This paper describes the creation of the AIRBUS-ATC corpus, which is a real-life, French-accented speech corpus of Air Traffic Control (ATC) communications (message exchanged between pilots and controllers) intended to build a robust ATC speech recognition engine. The corpus is currently composed of 59 hours of transcribed English audio, along with linguistic and meta-data annotations. It is intended to reach 100 hours by the end of the project. We describe ATC speech specificities, how the audio is collected, transcribed and what techniques were used to ensure transcription quality while limiting transcription costs. A detailed description of the corpus content (speaker gender, accent, role, type of control, speech turn duration) is given. Finally, preliminary results obtained with state-of-the-art speech recognition techniques support the idea that accent-specific corpora will play a pivotal role in building robust ATC speech recognition applications.},
  language = {en},
  urldate  = {2023-06-27},
  author   = {Delpech, Estelle and Laignelet, Marion and Pimm, Christophe and Raynal, Céline and Trzos, Michal and Arnold, Alexandre and Pronto, Dominique},
  month    = may,
  year     = {2018},
  file     = {Full Text PDF:/home/avandebrook/Zotero/storage/MIZYZ9FG/Delpech et al. - 2018 - A Real-life, French-accented Corpus of Air Traffic.pdf:application/pdf}
}

@misc{devlin_bert_2019,
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {http://arxiv.org/abs/1810.04805},
  doi        = {10.48550/arXiv.1810.04805},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2022-08-20},
  publisher  = {arXiv},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = may,
  year       = {2019},
  note       = {arXiv:1810.04805 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{falcon_pytorchlightning_2019,
  title   = {Pytorch lightning},
  volume  = {3},
  journal = {GitHub},
  author  = {Falcon, William A},
  year    = {2019}
}

@misc{gage_feb94_1994,
  title      = {{FEB94} {A} {New} {Algorithm} for {Data} {Compression}},
  shorttitle = {A {New} {Algorithm} for {Data} {Compression}},
  url        = {http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM},
  language   = {en},
  urldate    = {2023-09-29},
  author     = {Gage, Phillip},
  year       = {1994}
}

@misc{ginsburg_stochastic_2019,
  title    = {Stochastic {Gradient} {Methods} with {Layer}-wise {Adaptive} {Moments} for {Training} of {Deep} {Networks}},
  url      = {https://arxiv.org/abs/1905.11286v3},
  abstract = {We propose NovoGrad, an adaptive stochastic gradient descent method with layer-wise gradient normalization and decoupled weight decay. In our experiments on neural networks for image classification, speech recognition, machine translation, and language modeling, it performs on par or better than well tuned SGD with momentum and Adam or AdamW. Additionally, NovoGrad (1) is robust to the choice of learning rate and weight initialization, (2) works well in a large batch setting, and (3) has two times smaller memory footprint than Adam.},
  language = {en},
  urldate  = {2023-12-22},
  journal  = {arXiv.org},
  author   = {Ginsburg, Boris and Castonguay, Patrice and Hrinchuk, Oleksii and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Nguyen, Huyen and Zhang, Yang and Cohen, Jonathan M.},
  month    = may,
  year     = {2019},
  file     = {Full Text PDF:/home/avandebrook/Zotero/storage/BPNLAIRE/Ginsburg et al. - 2019 - Stochastic Gradient Methods with Layer-wise Adapti.pdf:application/pdf}
}

@misc{godfrey_air_1994,
  title     = {Air {Traffic} {Control} {Complete}},
  url       = {https://catalog.ldc.upenn.edu/LDC94S14A},
  abstract  = {The Air Traffic Control Corpus (ATC0) is comprised of recorded speech for use in supporting research and development activities in the area of robust speech recognition in domains similar to air traffic control (several speakers, noisy channels, relatively small vocabulary, constrained languaged, etc.) The audio data is composed of voice communication traffic between various controllers and pilots.},
  language  = {en},
  urldate   = {2022-07-16},
  publisher = {Linguistic Data Consortium},
  author    = {Godfrey, John J.},
  year      = {1994},
  keywords  = {Read}
}

@inproceedings{graglia_vocalise_2005,
  title      = {Vocalise: assessing the impact of data link technology on the {R}/{T} channel},
  volume     = {1},
  shorttitle = {Vocalise},
  doi        = {10.1109/DASC.2005.1563381},
  abstract   = {An inferential study aiming at seizing substantial gain data-link technology could yield in forthcoming years has been performed based on a solid knowledge of vocal medium use by pilots and controller. The study delivers a rich and argued analysis of the impact data link applications could have in the operational field, which type of gain could be awaited depending on the precise operational area. This study driven by hypothetical scenario has derived expected use of data link applications from a very detailed and up-to-date qualitative and quantitative database.},
  booktitle  = {24th {Digital} {Avionics} {Systems} {Conference}},
  author     = {Graglia, L. and Favennec, B. and Arnoux, A.},
  month      = oct,
  year       = {2005},
  note       = {ISSN: 2155-7209},
  keywords   = {Databases, Performance gain, Solids},
  pages      = {5.C.2--51},
  file       = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/ECE592TM/1563381.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/CV5ZZ52E/Graglia et al. - 2005 - Vocalise assessing the impact of data link techno.pdf:application/pdf}
}

@inproceedings{graves_connectionist_2006,
  address    = {New York, NY, USA},
  series     = {{ICML} '06},
  title      = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  isbn       = {978-1-59593-383-6},
  shorttitle = {Connectionist temporal classification},
  url        = {https://dl.acm.org/doi/10.1145/1143844.1143891},
  doi        = {10.1145/1143844.1143891},
  abstract   = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  urldate    = {2023-12-19},
  booktitle  = {Proceedings of the 23rd international conference on {Machine} learning},
  publisher  = {Association for Computing Machinery},
  author     = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
  month      = jun,
  year       = {2006},
  pages      = {369--376},
  file       = {Full Text PDF:/home/avandebrook/Zotero/storage/2L734JGR/Graves et al. - 2006 - Connectionist temporal classification labelling u.pdf:application/pdf}
}

@misc{guo_comparative_2022,
  title     = {A {Comparative} {Study} of {Speaker} {Role} {Identification} in {Air} {Traffic} {Communication} {Using} {Deep} {Learning} {Approaches}},
  url       = {http://arxiv.org/abs/2111.02041},
  doi       = {10.48550/arXiv.2111.02041},
  abstract  = {Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56\%, and 98.08\% accuracy, respectively.},
  urldate   = {2022-08-25},
  publisher = {arXiv},
  author    = {Guo, Dongyue and Zhang, Jianwei and Yang, Bo and Lin, Yi},
  month     = aug,
  year      = {2022},
  note      = {arXiv:2111.02041 [cs, eess]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{gurluk_assistant_2015,
  title     = {Assistant based speech recognition - another pair of eyes for the {Arrival} {Manager}},
  doi       = {10.1109/DASC.2015.7311396},
  abstract  = {Nowadays Arrival Managers (AMAN) are available to produce efficient inbound traffic sequences and to create guidance advisories for optimized approaches. Information about deviations from the planned sequence is exchanged between controller and pilot via radio communication. The AMAN is only able to derive these deviations from the radar data. Using radar data as single input sensor, however, results in adaptation delays of 30 seconds and more - and even worse, the controllers' intent is still missing. The AcListant® AMAN (Active Listening Assistant) [1] has shown for the Dusseldorf Approach Area how to avoid this sensor delay by analyzing the controller-pilot-communication and using the gained information as an additional sensor. An Assistant Based Speech Recognition system (ABSR) is embedded in an AMAN, which provides a dynamic minimized world model to the speech recognizer. Validation trials were performed from February to March 2015 with seven male and four female air traffic controllers from Dusseldorf, Frankfurt, Munich, Vienna, and Prague. Depending on the accepted rejection rate of the speech recognizer, recognition rates between 90\% and 95\% were achieved, whereas without ABSR only rates between 58\% and 83\% were possible. Furthermore ABSR significantly reduces the deviation between the controllers' plan and the plan of the AMAN and, at the same time, significantly reduces the controllers workload.},
  booktitle = {2015 {IEEE}/{AIAA} 34th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
  author    = {Gürlük, Hejar and Helmke, Hartmut and Wies, Matthias and Ehr, Heiko and Kleinert, Matthias and Mühlhausen, Thorsten and Muth, Kathleen and Ohneiser, Oliver},
  month     = sep,
  year      = {2015},
  note      = {ISSN: 2155-7209},
  keywords  = {Speech recognition, Air traffic control, Aircraft, Speech, Context, Radar, Trajectory},
  pages     = {3B6--1--3B6--14},
  file      = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/6MJGBSVH/7311396.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/HMJ2DC9L/Gürlük et al. - 2015 - Assistant based speech recognition - another pair .pdf:application/pdf}
}

@misc{han_contextnet_2020,
  title      = {{ContextNet}: {Improving} {Convolutional} {Neural} {Networks} for {Automatic} {Speech} {Recognition} with {Global} {Context}},
  shorttitle = {{ContextNet}},
  url        = {http://arxiv.org/abs/2005.03191},
  doi        = {10.48550/arXiv.2005.03191},
  abstract   = {Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1\%/4.6\% without external language model (LM), 1.9\%/4.1\% with LM and 2.9\%/7.0\% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0\%/4.6\% with LM and 3.9\%/11.3\% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.},
  urldate    = {2022-07-13},
  publisher  = {arXiv},
  author     = {Han, Wei and Zhang, Zhengdong and Zhang, Yu and Yu, Jiahui and Chiu, Chung-Cheng and Qin, James and Gulati, Anmol and Pang, Ruoming and Wu, Yonghui},
  month      = may,
  year       = {2020},
  note       = {arXiv:2005.03191 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Read}
}

@misc{hannun_deep_2014,
  title      = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
  shorttitle = {Deep {Speech}},
  url        = {http://arxiv.org/abs/1412.5567},
  doi        = {10.48550/arXiv.1412.5567},
  abstract   = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
  urldate    = {2023-12-20},
  publisher  = {arXiv},
  author     = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  month      = dec,
  year       = {2014},
  note       = {arXiv:1412.5567 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{helmke_assistant-based_2015,
  title  = {Assistant-based speech recognition for {ATM} applications},
  author = {Helmke, Hartmut and Rataj, Jürgen and Mühlhausen, Thorsten and Ohneiser, Oliver and Ehr, Heiko and Kleinert, Matthias and Oualil, Youssef and Schulder, Marc and Klakow, D},
  year   = {2015},
  file   = {Helmke et al. - 2015 - Assistant-based speech recognition for ATM applica.pdf:/home/avandebrook/Zotero/storage/K2S67M75/Helmke et al. - 2015 - Assistant-based speech recognition for ATM applica.pdf:application/pdf}
}

@inproceedings{helmke_machine_2020,
  title     = {Machine {Learning} of {Air} {Traffic} {Controller} {Command} {Extraction} {Models} for {Speech} {Recognition} {Applications}},
  doi       = {10.1109/DASC50938.2020.9256484},
  abstract  = {Increasing digitization and automation is a widely accepted method to cope with the challenges of constantly increasing air traffic. The analogue communication of air traffic controllers (ATCo) to pilots has been excluded so far from the digitization process. However, the content of this communication is of decisive importance for various automation systems. Although Assistant Based Speech Recognition (ABSR) has recently significantly improved the recognition performance and, therefore, enables the digitization of ATCo-pilot-communication, its adaptation to other airports is a critical and costly process, This is even more important, if ATCos tend to deviate from the published ICAO phraseology: “start reducing to two fifty” instead of “reduce two five zero knots” is just an example. User acceptance requires that these deviations are also correctly recognized. Therefore, this paper presents an approach, which automatically learns a so-called Command Extraction Model from labelled controller utterances. The initial Command Extraction Model without learning only covers 60\% of the commands, whereas the automatically learned Command Extraction Model covers more than 98\%. With just six hours of training data we could achieve 94\%.},
  booktitle = {2020 {AIAA}/{IEEE} 39th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
  author    = {Helmke, Hartmut and Kleinert, Matthias and Ohneiser, Oliver and Ehr, Heiko and Shetty, Shruthi},
  month     = oct,
  year      = {2020},
  note      = {ISSN: 2155-7209},
  keywords  = {Speech recognition, Ontologies, Adaptation models, Airports, Annotation, Annotations, Atmospheric modeling, Automatic Speech Recognition, Controller Command Extraction Model, Engines, Machine Learning, Ontology},
  pages     = {1--9},
  file      = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/LAKSXIXQ/9256484.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/CU7VRX8W/Helmke et al. - 2020 - Machine Learning of Air Traffic Controller Command.pdf:application/pdf}
}

@article{helmke_quantifying_2017,
  title   = {Quantifying the benefits of speech recognition for an air traffic management application},
  journal = {Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2017},
  author  = {Helmke, Hartmut and Oualil, Youssef and Schulder, Marc},
  year    = {2017},
  note    = {Publisher: TUDpress, Dresden},
  pages   = {114--121}
}

@inproceedings{helmke_readback_2021,
  title  = {Readback error detection by automatic speech recognition to increase {ATM} safety},
  author = {Helmke, Hartmut and Kleinert, Matthias and Shetty, Shruthi and Ohneiser, Oliver and Ehr, Heiko and Arilíusson, Hörður and Simiganoschi, Teodor S and Prasad, Amrutha and Motlicek, Petr and Veselý, Karel},
  year   = {2021},
  pages  = {20--23},
  file   = {Helmke et al. - 2021 - Readback error detection by automatic speech recog.pdf:/home/avandebrook/Zotero/storage/F556RGIE/Helmke et al. - 2021 - Readback error detection by automatic speech recog.pdf:application/pdf}
}

@inproceedings{hofbauer_atcosim_2008,
  address   = {Marrakech, Morocco},
  title     = {The {ATCOSIM} {Corpus} of {Non}-{Prompted} {Clean} {Air} {Traffic} {Control} {Speech}},
  abstract  = {Air traffic control (ATC) is based on voice communication between pilots and controllers and uses a highly task and domain specific language. Due to this very reason, spoken language technologies for ATC require domain-specific corpora, of which only few exist to this day. The ATCOSIM Air Traffic Control Simulation Speech corpus is a speech database of non-prompted and clean ATC operator speech. It consists of ten hours of speech data, which were recorded in typical ATC control room conditions during ATC real-time simulations. The database includes orthographic transcriptions and additional information on speakers and recording sessions. The ATCOSIM corpus is publicly available and provided online free of charge. In this paper, we first give an overview of ATC related corpora and their shortcomings. We then show the difficulties in obtaining operational ATC speech recordings and propose the use of existing ATC real-time simulations. We describe the recording, transcription, production and validation process of the ATCOSIM corpus, and outline an application example for automatic speech recognition in the ATC domain.},
  urldate   = {2022-10-20},
  booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'08)},
  publisher = {European Language Resources Association (ELRA)},
  author    = {Hofbauer, Konrad and Petrik, Stefan and Hering, Horst},
  month     = may,
  year      = {2008}
}

@article{hunt_figures_1990,
  title    = {Figures of merit for assessing connected-word recognisers},
  volume   = {9},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/016763939090008W},
  doi      = {10.1016/0167-6393(90)90008-W},
  number   = {4},
  urldate  = {2023-12-22},
  journal  = {Speech Communication},
  author   = {Hunt, Melvyn J.},
  month    = aug,
  year     = {1990},
  keywords = {dynamic programming, performance assessment, Speech recognition},
  pages    = {329--336},
  file     = {ScienceDirect Snapshot:/home/avandebrook/Zotero/storage/QPBZPZ2D/016763939090008W.html:text/html}
}

@incollection{jech_chapter_1978,
  title     = {Chapter 1 {Axiomatic} {Set} {Theory}},
  volume    = {79},
  isbn      = {978-0-12-381950-5},
  url       = {https://linkinghub.elsevier.com/retrieve/pii/S0079816908611927},
  language  = {en},
  urldate   = {2023-12-03},
  booktitle = {Pure and {Applied} {Mathematics}},
  publisher = {Elsevier},
  author    = {Jech, Thomas},
  year      = {1978},
  doi       = {10.1016/S0079-8169(08)61192-7},
  pages     = {1--77}
}

@inproceedings{kleinert_automated_2021,
  title      = {Automated {Interpretation} of {Air} {Traffic} {Control} {Communication}: {The} {Journey} from {Spoken} {Words} to a {Deeper} {Understanding} of the {Meaning}},
  shorttitle = {Automated {Interpretation} of {Air} {Traffic} {Control} {Communication}},
  doi        = {10.1109/DASC52595.2021.9594387},
  abstract   = {Sophisticated Automatic Speech Recognition (ASR) technologies have become increasingly popular and are widely used in all domains over the years. Systems like Google Assistant, Siri®, Alexa® are integrated into our day-to-day lives. These systems offer a wide range of possible applications just by understanding human speech. However, in the Air Traffic Control (ATC) domain, even the most advanced simulators can just partially replace expensive pseudo-pilots. In spite of having a standardized ATC phraseology, it is still a major challenge to recognize and correctly understand the communication between air traffic controllers (ATCo) and pilots. This is because understanding an ATCo-pilot communication requires more than just transforming speech to a sequence of words. For most ATC applications, perfectly recognizing the sequence of words would not be useful, if the meaning behind the word sequence cannot be correctly interpreted. Recently, 20 European partners from Air Traffic Management (ATM) domain have agreed on a common set of rules, i.e., an ontology on how to transform the spoken words into ATC instructions that clearly define the meaning of the words and make them usable for different applications. In this paper, we present an extension of the mentioned ontology to make it usable for pilot speech as well. We also show some of the challenges faced in understanding the meaning of ATCo-pilot communication and describe our approach of tackling them. Furthermore, we present an algorithm to transform words automatically into ontology instructions and describe the interfaces used to ensure a consistent and reliable communication of ATC instructions. This interface includes, besides other information, plausibility values, different speakers, and ambiguous outputs.},
  booktitle  = {2021 {IEEE}/{AIAA} 40th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
  author     = {Kleinert, Matthias and Helmke, Hartmut and Shetty, Shruthi and Ohneiser, Oliver and Ehr, Heiko and Prasad, Amrutha and Motlicek, Petr and Harfmann, Julia},
  month      = oct,
  year       = {2021},
  note       = {ISSN: 2155-7209},
  keywords   = {Air traffic control, Internet, Ontologies, Aerospace electronics, air traffic control, ATC command ontology, command extraction, command recognition rate, Europe, JSON, language understanding, Reliability, Transforms},
  pages      = {1--9},
  file       = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/2JDBV6I5/9594387.html:text/html}
}

@inproceedings{kriman_quartznet_2020,
  title      = {Quartznet: {Deep} {Automatic} {Speech} {Recognition} with {1D} {Time}-{Channel} {Separable} {Convolutions}},
  shorttitle = {Quartznet},
  doi        = {10.1109/ICASSP40776.2020.9053889},
  abstract   = {We propose a new end-to-end neural acoustic model for automatic speech recognition. The model is composed of multiple blocks with residual connections between them. Each block consists of one or more modules with 1D time-channel separable convolutional layers, batch normalization, and ReLU layers. It is trained with CTC loss. The proposed network achieves near state-of-the-art accuracy on LibriSpeech and Wall Street Journal, while having fewer parameters than all competing models. We also demonstrate that this model can be effectively fine-tuned on new datasets.},
  booktitle  = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author     = {Kriman, Samuel and Beliaev, Stanislav and Ginsburg, Boris and Huang, Jocelyn and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Zhang, Yang},
  month      = may,
  year       = {2020},
  note       = {ISSN: 2379-190X},
  keywords   = {Acoustics, Automatic speech recognition, Conferences, Convolution, convolutional networks, depthwise separable convolution, Speech processing, time-channel separable convolution, Read},
  pages      = {6124--6128}
}

@misc{kuchaiev_nemo_2019,
  title      = {{NeMo}: a toolkit for building {AI} applications using {Neural} {Modules}},
  shorttitle = {{NeMo}},
  url        = {http://arxiv.org/abs/1909.09577},
  doi        = {10.48550/arXiv.1909.09577},
  abstract   = {NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI applications through re-usability, abstraction, and composition. NeMo is built around neural modules, conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition and natural language processing. Furthermore, NeMo provides built-in support for distributed training and mixed precision on latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo},
  urldate    = {2023-12-01},
  publisher  = {arXiv},
  author     = {Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and Castonguay, Patrice and Popova, Mariya and Huang, Jocelyn and Cohen, Jonathan M.},
  month      = sep,
  year       = {2019},
  note       = {arXiv:1909.09577 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/home/avandebrook/Zotero/storage/WAPY54DL/Kuchaiev et al. - 2019 - NeMo a toolkit for building AI applications using.pdf:application/pdf;arXiv.org Snapshot:/home/avandebrook/Zotero/storage/BCU8WBBX/1909.html:text/html}
}

@article{lechner_voice_2002,
  title      = {Voice recognition: software solutions in real-time {ATC} workstations},
  volume     = {17},
  issn       = {1557-959X},
  shorttitle = {Voice recognition},
  doi        = {10.1109/MAES.2002.1047373},
  abstract   = {Speech recognition features desired by air traffic controllers, such as the ability to use complex messages and address hundreds of individual aircraft could not be implemented a decade ago, but these tasks became possible with improved speech recognition engines and an increase in processing power and memory. Speech recognition was a key element in the air traffic controller (ATC) workstation used to support a Controller-Pilot Data Link Communications (CPDLC) system. Our work, under the direction of the Avionics Engineering Center at Ohio University, was in support of the Federal Aviation Administration's (FAA) Runway Incursion Reduction Program (RIRP) and the National Aeronautics and Space Administration's (NASA) Runway Incursion Prevention System (RIPS) conducted at the Dallas-Fort Worth International Airport (DFW). This paper examines the challenges and opportunities of developing voice recognition software solutions in ATC workstations using multiple dialects and accents, complex and varied grammars and terminology, accuracy, hardware restrictions, and user-training procedures.},
  number     = {11},
  journal    = {IEEE Aerospace and Electronic Systems Magazine},
  author     = {Lechner, A. and Mattson, P. and Ecker, K.},
  month      = nov,
  year       = {2002},
  note       = {Conference Name: IEEE Aerospace and Electronic Systems Magazine},
  keywords   = {Aerospace control, Aerospace electronics, Air traffic control, Aircraft propulsion, Communication system traffic control, Control systems, Engines, FAA, Speech recognition, Workstations},
  pages      = {11--16},
  file       = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/J4FFFK6W/1047373.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/GC5EL7WA/Lechner et al. - 2002 - Voice recognition software solutions in real-time.pdf:application/pdf}
}

@inproceedings{lee_unsupervised_2009,
  title     = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
  volume    = {22},
  url       = {https://proceedings.neurips.cc/paper/2009/hash/a113c1ecd3cace2237256f4c712f61b5-Abstract.html},
  abstract  = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  urldate   = {2023-12-20},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Lee, Honglak and Pham, Peter and Largman, Yan and Ng, Andrew},
  year      = {2009},
  file      = {Full Text PDF:/home/avandebrook/Zotero/storage/2X5FZ5YY/Lee et al. - 2009 - Unsupervised feature learning for audio classifica.pdf:application/pdf}
}

@misc{lewis_bart_2019,
  title      = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
  shorttitle = {{BART}},
  url        = {http://arxiv.org/abs/1910.13461},
  doi        = {10.48550/arXiv.1910.13461},
  abstract   = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  urldate    = {2022-08-21},
  publisher  = {arXiv},
  author     = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  month      = oct,
  year       = {2019},
  note       = {arXiv:1910.13461 [cs, stat]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{li_jasper_2019,
  title      = {Jasper: {An} {End}-to-{End} {Convolutional} {Neural} {Acoustic} {Model}},
  shorttitle = {Jasper},
  url        = {http://arxiv.org/abs/1904.03288},
  doi        = {10.48550/arXiv.1904.03288},
  abstract   = {In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95\% WER using a beam-search decoder with an external neural language model and 3.86\% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets.},
  urldate    = {2023-12-20},
  publisher  = {arXiv},
  author     = {Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan M. and Nguyen, Huyen and Gadde, Ravi Teja},
  month      = aug,
  year       = {2019},
  note       = {arXiv:1904.03288 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/home/avandebrook/Zotero/storage/UR5MDGQK/Li et al. - 2019 - Jasper An End-to-End Convolutional Neural Acousti.pdf:application/pdf;arXiv.org Snapshot:/home/avandebrook/Zotero/storage/J7VPSZDV/1904.html:text/html}
}

@misc{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  doi        = {10.48550/arXiv.1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2022-08-21},
  publisher  = {arXiv},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv:1907.11692 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{loshchilov_decoupled_2019,
  title     = {Decoupled {Weight} {Decay} {Regularization}},
  url       = {http://arxiv.org/abs/1711.05101},
  doi       = {10.48550/arXiv.1711.05101},
  abstract  = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  urldate   = {2023-11-24},
  publisher = {arXiv},
  author    = {Loshchilov, Ilya and Hutter, Frank},
  month     = jan,
  year      = {2019},
  note      = {arXiv:1711.05101 [cs, math]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control}
}

@inproceedings{maas_learning_2011,
  address   = {USA},
  series    = {{HLT} '11},
  title     = {Learning word vectors for sentiment analysis},
  isbn      = {978-1-932432-87-9},
  abstract  = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
  urldate   = {2022-07-16},
  booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Volume} 1},
  publisher = {Association for Computational Linguistics},
  author    = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  month     = jun,
  year      = {2011},
  pages     = {142--150}
}

@article{madeira_machine_2021,
  title     = {Machine {Learning} and {Natural} {Language} {Processing} for {Prediction} of {Human} {Factors} in {Aviation} {Incident} {Reports}},
  volume    = {8},
  copyright = {© 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  url       = {https://www.proquest.com/docview/2524211382/abstract/5B09B07DC1084F4EPQ/1},
  doi       = {10.3390/aerospace8020047},
  abstract  = {In the aviation sector, human factors are the primary cause of safety incidents. Intelligent prediction systems, which are capable of evaluating human state and managing risk, have been developed over the years to identify and prevent human factors. However, the lack of large useful labelled data has often been a drawback to the development of these systems. This study presents a methodology to identify and classify human factor categories from aviation incident reports. For feature extraction, a text pre-processing and Natural Language Processing (NLP) pipeline is developed. For data modelling, semi-supervised Label Spreading (LS) and supervised Support Vector Machine (SVM) techniques are considered. Random search and Bayesian optimization methods are applied for hyper-parameter analysis and the improvement of model performance, as measured by the Micro F1 score. The best predictive models achieved a Micro F1 score of 0.900, 0.779, and 0.875, for each level of the taxonomic framework, respectively. The results of the proposed method indicate that favourable predicting performances can be achieved for the classification of human factors based on text data. Notwithstanding, a larger data set would be recommended in future research.},
  language  = {English},
  number    = {2},
  urldate   = {2022-08-25},
  journal   = {Aerospace},
  author    = {Madeira, Tomás and Melício, Rui and Valério, Duarte and Santos, Luis},
  year      = {2021},
  note      = {Num Pages: 47
               Place: Basel, Switzerland
               Publisher: MDPI AG},
  keywords  = {aviation incident reports, aviation safety, human factors, machine learning, natural language processing, prediction},
  pages     = {47}
}

@misc{majumdar_citrinet_2021,
  title      = {Citrinet: {Closing} the {Gap} between {Non}-{Autoregressive} and {Autoregressive} {End}-to-{End} {Models} for {Automatic} {Speech} {Recognition}},
  shorttitle = {Citrinet},
  url        = {http://arxiv.org/abs/2104.01721},
  doi        = {10.48550/arXiv.2104.01721},
  abstract   = {We propose Citrinet - a new end-to-end convolutional Connectionist Temporal Classification (CTC) based automatic speech recognition (ASR) model. Citrinet is deep residual neural model which uses 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation. The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models. We evaluate Citrinet on LibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English speech datasets. Citrinet accuracy on these datasets is close to the best autoregressive Transducer models.},
  urldate    = {2023-11-21},
  publisher  = {arXiv},
  author     = {Majumdar, Somshubra and Balam, Jagadeesh and Hrinchuk, Oleksii and Lavrukhin, Vitaly and Noroozi, Vahid and Ginsburg, Boris},
  month      = apr,
  year       = {2021},
  note       = {arXiv:2104.01721 [eess]},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{mayer_electronic_1981,
  title     = {Electronic {Voice} {Communications} {Improvements} for {Army} {Aircraft}},
  booktitle = {Aural {Communications} in {Aviation}},
  author    = {Mayer, Mitchell S. and Lindburg, Arthur W.},
  year      = {1981},
  pages     = {35--46}
}

@misc{merity_pointer_2016,
  title    = {Pointer {Sentinel} {Mixture} {Models}},
  url      = {https://arxiv.org/abs/1609.07843v1},
  abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
  language = {en},
  urldate  = {2023-10-20},
  journal  = {arXiv.org},
  author   = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  month    = sep,
  year     = {2016}
}

@article{moere_implementing_2009,
  title    = {Implementing {ICAO} language proficiency requirements in the {Versant} {Aviation} {English} {Test}},
  volume   = {32},
  issn     = {0155-0640, 1833-7139},
  url      = {https://www.jbe-platform.com/content/journals/10.2104/aral0927},
  doi      = {10.2104/aral0927},
  abstract = {This paper discusses the development of an assessment to satisfy the International Civil Aviation Organization (ICAO) Language Proficiency Requirements. The Versant Aviation English Test utilizes speech recognition technology and a computerized testing platform, such that test administration and scoring are fully automated. Developed in collaboration with the U.S. Federal Aviation Administration, this 25-minute test is delivered via a telephone or computer. Two issues of interest are discussed. The first concerns the practicalities of assessing candidates in each of six separate dimensions of spoken proficiency: Pronunciation, Structure, Vocabulary, Fluency, Comprehension, and Interactions. Although an automated scoring system can objectively segregate these skills, we question whether human raters have the capacity to do this in oral interviews. The second issue discussed is how an automated test can provide a valid assessment of spoken interactions. Tasks were designed to simulate the information exchange between pilots and controllers on which candidates’ proficiency in ‘Interactions’ could be measured, for example, by eliciting functions such as correcting miscommunications and providing clarification. It is argued that candidate ability can be probed and estimated in a fair and standardized way by presenting a series of independent items which are targeted in difficulty at the various ICAO levels.},
  language = {en},
  number   = {3},
  urldate  = {2023-06-28},
  journal  = {Australian Review of Applied Linguistics},
  author   = {Moere, Alistair Van and Suzuki, Masanori and Downey, Ryan and Cheng, Jian},
  month    = jan,
  year     = {2009},
  note     = {Publisher: John Benjamins},
  pages    = {27.1--27.17},
  file     = {Full Text:/home/avandebrook/Zotero/storage/DTNJ767T/Moere et al. - 2009 - Implementing ICAO language proficiency requirement.pdf:application/pdf;Snapshot:/home/avandebrook/Zotero/storage/FKE59LQR/aral0927.html:text/html}
}

@techreport{money_aural_1981,
  type     = {Conference {Proceedings}},
  title    = {Aural {Communication} in {Aviation}.},
  url      = {https://apps.dtic.mil/sti/citations/ADA103395},
  abstract = {The importance of aural information in aerospace operations is secondary only to visual information yet, despite the dependence of military operations on reliable voice communication and the effective use of audio warnings, many of the systems currently employed have serious shortcomings and do not reflect the considerable research effort that has been expended in this area. The Aerospace Medical Panel AMP of AGARD considered it to be timely to discuss this topic, so that there could be a wider understanding of the problems of speech recognition in aircraft noise, the causal mechanisms of subsequent aviator hearing loss and of the latest developments in research to combat the shortcomings in voice communications systems.},
  language = {en},
  urldate  = {2023-06-28},
  author   = {Money, K.E.},
  year     = {1981},
  note     = {Section: Technical Reports},
  pages    = {190},
  file     = {Aural Communication in Aviation..pdf:/home/avandebrook/Zotero/storage/9RX3TEMV/Aural Communication in Aviation..pdf:application/pdf;Snapshot:/home/avandebrook/Zotero/storage/ULERRPFU/ADA103395.html:text/html}
}

@inproceedings{mosko_clear_1981,
  title     = {"{Clear} {Speech}": {A} {Strategem} for {Improving} {Radio} {Communications} and {Automatic} {Speech} {Recognition} in {Noise}},
  booktitle = {Aural {Communications} in {Aviation}},
  author    = {Mosko, James D.},
  year      = {1981},
  pages     = {29--32}
}

@misc{noauthor_american_2011,
  edition  = {5},
  title    = {The {American} {Heritage} {Dictionary} of the {English} {Language}},
  url      = {https://ahdictionary.com/word/search.html?q=word},
  language = {en},
  urldate  = {2023-12-07},
  journal  = {The American Heritage Dictionary of the English Language},
  month    = nov,
  year     = {2011}
}

@incollection{north_application_1984,
  series    = {General {Aviation} {Technology} {Conference}},
  title     = {Application of speech recognition and synthesis in the general aviation cockpit},
  url       = {https://arc.aiaa.org/doi/10.2514/6.1984-2239},
  urldate   = {2023-06-28},
  booktitle = {General {Aviation} {Technology} {Conference}},
  publisher = {American Institute of Aeronautics and Astronautics},
  author    = {North, R. and Mountford, S. and Bergeron, H.},
  month     = jul,
  year      = {1984},
  doi       = {10.2514/6.1984-2239},
  keywords  = {Air Forces, Avionics Systems, Cockpit, Computing, General Aviation, Linkages, NASA Langley Research Center, Pilot, RADAR},
  file      = {Full Text PDF:/home/avandebrook/Zotero/storage/VQ278MNH/North et al. - 1984 - Application of speech recognition and synthesis in.pdf:application/pdf}
}

@inproceedings{north_systems_1984,
  title  = {Systems concept for speech technology application in general aviation},
  author = {NORTH, R and Bergeron, Hugh},
  year   = {1984},
  pages  = {2639},
  file   = {NORTH and Bergeron - 1984 - Systems concept for speech technology application .pdf:/home/avandebrook/Zotero/storage/57XIF7KC/NORTH and Bergeron - 1984 - Systems concept for speech technology application .pdf:application/pdf}
}

@book{paltridge_handbook_2013,
  address   = {Chichester, West Sussex},
  title     = {The {Handbook} of {English} for {Specific} {Purposes}},
  isbn      = {978-1-118-29058-3 978-1-118-29059-0 978-1-118-29060-6},
  language  = {en},
  publisher = {Wiley-Blackwell},
  editor    = {Paltridge, Brian and Starfield, Sue},
  year      = {2013},
  keywords  = {Anthropological ethics, Handbooks, manuals, etc, SOCIAL SCIENCE / Anthropology / Cultural}
}


@inproceedings{paszke_pytorch_2019,
  title      = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
  volume     = {32},
  shorttitle = {{PyTorch}},
  abstract   = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
                In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
                We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  urldate    = {2023-11-30},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year       = {2019}
}


@article{pedregosa_scikit-learn_2011,
  title   = {Scikit-learn: {Machine} learning in {Python}},
  volume  = {12},
  issn    = {1532-4435},
  journal = {the Journal of machine Learning research},
  author  = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
  year    = {2011},
  note    = {Publisher: JMLR. org},
  pages   = {2825--2830},
  file    = {Pedregosa et al. - 2011 - Scikit-learn Machine learning in Python.pdf:/home/avandebrook/Zotero/storage/9XDDCW86/Pedregosa et al. - 2011 - Scikit-learn Machine learning in Python.pdf:application/pdf}
}

@inproceedings{pellegrini_airbus_2019,
  title      = {The {Airbus} {Air} {Traffic} {Control} speech recognition 2018 challenge: towards {ATC} automatic transcription and call sign detection},
  shorttitle = {The {Airbus} {Air} {Traffic} {Control} speech recognition 2018 challenge},
  url        = {http://arxiv.org/abs/1810.12614},
  doi        = {10.21437/Interspeech.2019-1962},
  abstract   = {In this paper, we describe the outcomes of the challenge organized and run by Airbus and partners in 2018. The challenge consisted of two tasks applied to Air Traffic Control (ATC) speech in English: 1) automatic speech-to-text transcription, 2) call sign detection (CSD). The registered participants were provided with 40 hours of speech along with manual transcriptions. Twenty-two teams submitted predictions on a five hour evaluation set. ATC speech processing is challenging for several reasons: high speech rate, foreign-accented speech with a great diversity of accents, noisy communication channels. The best ranked team achieved a 7.62\% Word Error Rate and a 82.41\% CSD F1-score. Transcribing pilots' speech was found to be twice as harder as controllers' speech. Remaining issues towards solving ATC ASR are also discussed.},
  urldate    = {2023-04-06},
  booktitle  = {Interspeech 2019},
  author     = {Pellegrini, Thomas and Farinas, Jérôme and Delpech, Estelle and Lancelot, François},
  month      = sep,
  year       = {2019},
  note       = {arXiv:1810.12614 [cs, eess]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  pages      = {2993--2997}
}

@article{radford_improving_2018,
  title  = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year   = {2018},
  note   = {Publisher: OpenAI}
}

@article{radford_language_2019,
  title   = {Language models are unsupervised multitask learners},
  volume  = {1},
  number  = {8},
  journal = {OpenAI blog},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year    = {2019},
  pages   = {9}
}

@misc{raffel_exploring_2020,
  title     = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
  url       = {http://arxiv.org/abs/1910.10683},
  doi       = {10.48550/arXiv.1910.10683},
  abstract  = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  urldate   = {2022-08-21},
  publisher = {arXiv},
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  month     = jul,
  year      = {2020},
  note      = {arXiv:1910.10683 [cs, stat]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{ragnarsdottir_language_2003,
  title     = {Language technology in air traffic control},
  volume    = {1},
  doi       = {10.1109/DASC.2003.1245815},
  abstract  = {Voice communication is a volatile part of Air Traffic Control (ATC). According to research, on average one miscommunication happens every hour per radio frequency where there is frequent communication such as in TRACON. ICAO puts great emphasis on improving communication in ATC. This article proposes that a language technology system (LTS) can make communication between controller and pilot more reliable and efficient, thus improving safety in aviation. An LTS can for example detect readback errors. It can also directly feed data from the voice recognizer on XML (eXtensible Markup Language) form into a flight data processing system or interact with it as we show. By interviewing air traffic controllers and studying the literature, we identified several examples of use of language technology in ATC. As an example we take a system to support controllers in their work by making the LTS give warnings when discrepancy is found in the communications between controller and pilot. This system is not meant to control the airspace autonomously. Latest advances in language technology have enabled the development of such a system. The functionality of the proposed LTS is described using scenarios and sequence diagrams. A demonstration conversational agent using Hex Technology was implemented. A Wizard of Oz usability test was administered to seven controllers. Their attitude to the agent was positive and indicates that there is reason for further research. The performance and error logs of the agent and voice server were analyzed and give guidance on further development of a fully functioning language technology system for air traffic control.},
  booktitle = {Digital {Avionics} {Systems} {Conference}, 2003. {DASC} '03. {The} 22nd},
  author    = {Ragnarsdottir, M.D. and Waage, H. and Hvannberg, E.T.},
  month     = oct,
  year      = {2003},
  keywords  = {Speech recognition, Air traffic control, Aircraft, Grammar, Natural language processing, Safety, Speech},
  pages     = {2.E.2--21--13 vol.1},
  file      = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/G7XJY9XB/5731067.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/N9QWK5YU/Ragnarsdottir et al. - 2003 - Language technology in air traffic control.pdf:application/pdf}
}

@article{sahidullah_design_2012,
  title    = {Design, analysis and experimental evaluation of block based transformation in {MFCC} computation for speaker recognition},
  volume   = {54},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167639311001622},
  doi      = {10.1016/j.specom.2011.11.004},
  abstract = {Standard Mel frequency cepstrum coefficient (MFCC) computation technique utilizes discrete cosine transform (DCT) for decorrelating log energies of filter bank output. The use of DCT is reasonable here as the covariance matrix of Mel filter bank log energy (MFLE) can be compared with that of highly correlated Markov-I process. This full-band based MFCC computation technique where each of the filter bank output has contribution to all coefficients, has two main disadvantages. First, the covariance matrix of the log energies does not exactly follow Markov-I property. Second, full-band based MFCC feature gets severely degraded when speech signal is corrupted with narrow-band channel noise, though few filter bank outputs may remain unaffected. In this work, we have studied a class of linear transformation techniques based on block wise transformation of MFLE which effectively decorrelate the filter bank log energies and also capture speech information in an efficient manner. A thorough study has been carried out on the block based transformation approach by investigating a new partitioning technique that highlights associated advantages. This article also reports a novel feature extraction scheme which captures complementary information to wide band information; that otherwise remains undetected by standard MFCC and proposed block transform (BT) techniques. The proposed features are evaluated on NIST SRE databases using Gaussian mixture model-universal background model (GMM-UBM) based speaker recognition system. We have obtained significant performance improvement over baseline features for both matched and mismatched condition, also for standard and narrow-band noises. The proposed method achieves significant performance improvement in presence of narrow-band noise when clubbed with missing feature theory based score computation scheme.},
  number   = {4},
  urldate  = {2023-12-20},
  journal  = {Speech Communication},
  author   = {Sahidullah, Md. and Saha, Goutam},
  month    = may,
  year     = {2012},
  keywords = {Block transform, Correlation matrix, DCT, Decorrelation technique, Linear transformation, MFCC, Missing feature theory, Narrow-band noise, Speaker recognition},
  pages    = {543--565},
  file     = {ScienceDirect Snapshot:/home/avandebrook/Zotero/storage/FY8WLZH7/S0167639311001622.html:text/html}
}

@article{schafer_context-sensitive_2000,
  title  = {Context-sensitive speech recognition in the air traffic control simulation},
  author = {Schäfer, Dirk},
  year   = {2000},
  note   = {Publisher: Citeseer},
  file   = {Schäfer - 2000 - Context-sensitive speech recognition in the air tr.pdf:/home/avandebrook/Zotero/storage/Y5BSCMM9/Schäfer - 2000 - Context-sensitive speech recognition in the air tr.pdf:application/pdf}
}

@inproceedings{schuster_japanese_2012,
  title     = {Japanese and {Korean} voice search},
  url       = {https://ieeexplore.ieee.org/document/6289079},
  doi       = {10.1109/ICASSP.2012.6289079},
  abstract  = {This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.},
  urldate   = {2023-11-16},
  booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  month     = mar,
  year      = {2012},
  note      = {ISSN: 2379-190X},
  pages     = {5149--5152}
}

@misc{segura_hiwire_2007,
  title      = {The {HIWIRE} database, a noisy and non-native {English} speech corpus for cockpit communication. http://www.hiwire.org},
  shorttitle = {The {HIWIRE} database, a noisy and non-native {English} speech corpus for cockpit communication},
  abstract   = {The HIWIRE database was collected to investigate human-machine spoken dialogue communication in the cockpit. The two main adverse conditions encountered in aeronautical appli-cations have been simulated in this database, namely, cockpit noise and non-native speech. The database collection process, application domain (command and control in the cockpit) and speaker population are described in detail. In addition, base-line speech recognition experiments are presented for the robust speech recognition task and the non-native tasks. Model adapta-tion results are also included in the paper, to serve as a baseline for further research in this area. Index Terms: robust speech recognition, non-native speech recognition},
  author     = {Segura, J. C. and Ehrette, T. and Potamianos, A. and Fohr, D. and Illina, I. and Breton, P.-a and Clot, V. and Gemello, R. and Matassoni, M. and Maragos, P.},
  year       = {2007},
  keywords   = {Unread},
  file       = {Citeseer - Full Text PDF:/home/avandebrook/Zotero/storage/BE4PEHZ2/Segura et al. - 2007 - The HIWIRE database, a noisy and non-native Englis.pdf:application/pdf;Citeseer - Snapshot:/home/avandebrook/Zotero/storage/38PJJT9Y/summary.html:text/html}
}

@misc{sennrich_neural_2016,
  title     = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
  url       = {http://arxiv.org/abs/1508.07909},
  doi       = {10.48550/arXiv.1508.07909},
  abstract  = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  urldate   = {2022-07-13},
  publisher = {arXiv},
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1508.07909 [cs]},
  keywords  = {Computer Science - Computation and Language, Unread}
}

@inproceedings{shi_end--end_2022,
  address   = {Cham},
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {An {End}-to-{End} {Conformer}-{Based} {Speech} {Recognition} {Model} for {Mandarin} {Radiotelephony} {Communications} in {Civil} {Aviation}},
  isbn      = {978-3-031-20233-9},
  doi       = {10.1007/978-3-031-20233-9_34},
  abstract  = {In civil aviation radiotelephony communications, misunderstandings between air traffic controllers and flight crews can result in serious aviation accidents. Automatic semantic verification is a promising assistant solution to decrease miscommunication, thanks to advancements in speech and language processing. Unfortunately, existing general speech recognition models are ineffective when it comes to capturing contextual long-distance dependent local similarity features in radiotelephony communications. To address these problems, this paper proposes an end-to-end Conformer-based multi-task learning speech recognition model for Mandarin radiotelephony communications in civil aviation. The Conformer model improves local information capture while retaining the global information modeling capabilities of contextual long-distance dependencies, owing to the introduction of the convolution module to the Transformer model. Meanwhile, multi-task learning is used to further improve performance by combining connectionist temporal classification (CTC) and attention-based encoder-decoder (AED) models. The experimental results show that the proposed model can perform global and local acoustic modeling effectively, making it particularly suitable for extracting acoustic features of Mandarin civil aviation radiotelephony communications.},
  language  = {en},
  booktitle = {Biometric {Recognition}},
  publisher = {Springer Nature Switzerland},
  author    = {Shi, Yihua and Ma, Guanglin and Ren, Jin and Zhang, Haigang and Yang, Jinfeng},
  editor    = {Deng, Weihong and Feng, Jianjiang and Huang, Di and Kan, Meina and Sun, Zhenan and Zheng, Fang and Wang, Wenfeng and He, Zhaofeng},
  year      = {2022},
  keywords  = {Automatic speech recognition, Conformer, End-to-end, Mandarin civil aviation radiotelephony communication, Multi-task learning},
  pages     = {335--347},
  file      = {Shi et al. - 2022 - An End-to-End Conformer-Based Speech Recognition M.pdf:/home/avandebrook/Zotero/storage/E25GVVX8/Shi et al. - 2022 - An End-to-End Conformer-Based Speech Recognition M.pdf:application/pdf}
}

@article{smidl_air_2019,
  title    = {Air traffic control communication ({ATCC}) speech corpora and their use for {ASR} and {TTS} development},
  volume   = {53},
  issn     = {1574-0218},
  url      = {https://doi.org/10.1007/s10579-019-09449-5},
  doi      = {10.1007/s10579-019-09449-5},
  abstract = {The paper introduces the motivation for creating dedicated speech corpora of air traffic control communication, describes in detail the process of preparation of corpora for both automatic speech recognition and text-to-speech synthesis, presents an illustrative example of speech recognition system developed using the automatic speech recognition corpora and finally describes the technical aspects of the data and the distribution channel.},
  language = {en},
  number   = {3},
  urldate  = {2022-07-14},
  journal  = {Language Resources and Evaluation},
  author   = {Šmídl, Luboš and Švec, Jan and Tihelka, Daniel and Matoušek, Jindřich and Romportl, Jan and Ircing, Pavel},
  month    = sep,
  year     = {2019},
  keywords = {Air traffic control communication, Read, Speech corpus, Speech recognition, Text-to-speech},
  pages    = {449--464}
}

@book{srinivasamurthy_semi-supervised_2017,
  title    = {Semi-supervised {Learning} with {Semantic} {Knowledge} {Extraction} for {Improved} {Speech} {Recognition} in {Air} {Traffic} {Control}},
  abstract = {Automatic Speech Recognition (ASR) can introduce higher levels of automation into Air Traffic Control (ATC), where spoken language is still the predominant form of communication. While ATC uses standard phraseology and a limited vocabulary, we need to adapt the speech recognition systems to local acoustic conditions and vocabularies at each airport to reach optimal performance. Due to continuous operation of ATC systems, a large and increasing amount of untranscribed speech data is available, allowing for semi-supervised learning methods to build and adapt ASR models. In this paper, we first identify the challenges in building ASR systems for specific ATC areas and propose to utilize out-of-domain data to build baseline ASR models. Then we explore different methods of data selection for adapting baseline models by exploiting the continuously increasing untranscribed data. We develop a basic approach capable of exploiting semantic representations of ATC commands. We achieve relative improvement in both word error rate (23.5\%) and concept error rates (7\%) when adapting ASR models to different ATC conditions in a semi-supervised manner},
  editor   = {Srinivasamurthy, Ajay and Motlicek, Petr and Himawan, Ivan and Szaszak, Gyorgy and Oualil, Youssef and Helmke, Hartmut},
  year     = {2017},
  doi      = {10.21437/Interspeech.2017-1446},
  note     = {Meeting Name: Proceedings of Interspeech 2017}
}

@article{szoke_detecting_2021,
  title    = {Detecting {English} {Speech} in the {Air} {Traffic} {Control} {Voice} {Communication}},
  url      = {https://arxiv.org/abs/2104.02332v1},
  doi      = {10.48550/arXiv.2104.02332},
  abstract = {We launched a community platform for collecting the ATC speech world-wide in the ATCO2 project. Filtering out unseen non-English speech is one of the main components in the data processing pipeline. The proposed English Language Detection (ELD) system is based on the embeddings from Bayesian subspace multinomial model. It is trained on the word confusion network from an ASR system. It is robust, easy to train, and light weighted. We achieved 0.0439 equal-error-rate (EER), a 50\% relative reduction as compared to the state-of-the-art acoustic ELD system based on x-vectors, in the in-domain scenario. Further, we achieved an EER of 0.1352, a 33\% relative reduction as compared to the acoustic ELD, in the unseen language (out-of-domain) condition. We plan to publish the evaluation dataset from the ATCO2 project.},
  language = {en},
  urldate  = {2022-11-05},
  author   = {Szoke, Igor and Kesiraju, Santosh and Novotny, Ondrej and Kocour, Martin and Vesely, Karel and Cernocky, Jan "Honza"},
  month    = apr,
  year     = {2021}
}

@article{tanguy_natural_2016,
  series     = {Natural {Language} {Processing} and {Text} {Analytics} in {Industry}},
  title      = {Natural language processing for aviation safety reports: {From} classification to interactive analysis},
  volume     = {78},
  issn       = {0166-3615},
  shorttitle = {Natural language processing for aviation safety reports},
  url        = {https://www.sciencedirect.com/science/article/pii/S0166361515300464},
  doi        = {10.1016/j.compind.2015.09.005},
  abstract   = {In this paper we describe the different NLP techniques designed and used in collaboration between the CLLE-ERSS research laboratory and the CFH/Safety Data company to manage and analyse aviation incident reports. These reports are written every time anything abnormal occurs during a civil air flight. Although most of them relate routine problems, they are a valuable source of information about possible sources of greater danger. These texts are written in plain language, show a wide range of linguistic variation (telegraphic style overcrowded by acronyms or standard prose) and exist in different languages, even for a single company/country (although our main focus is on English and French). In addition to their variety, their sheer quantity (e.g. 600/month for a large airline company) clearly requires the use of advanced NLP and text mining techniques in order to extract useful information from them. Although this context and objectives seem to indicate that standard NLP techniques can be applied in a straightforward manner, innovative techniques are required to handle the specifics of aviation report text and the complex classification systems. We present several tools that aim at a better access to this data (classification and information retrieval), and help aviation safety experts in their analyses (data/text mining and interactive analysis). Some of these tools are currently in test or in use both at the national and international levels, by airline companies as well as by regulation authorities (DGAC,11Direction Générale de l’Aviation Civile. EASA,22European Aviation Safety Agency. ICAO33International Civil Aviation Organization.).},
  language   = {en},
  urldate    = {2022-08-25},
  journal    = {Computers in Industry},
  author     = {Tanguy, Ludovic and Tulechki, Nikola and Urieli, Assaf and Hermann, Eric and Raynal, Céline},
  month      = may,
  year       = {2016},
  keywords   = {Aviation, Document classification, NLP, Safety reports, Text mining},
  pages      = {80--95}
}

@article{vaswani_attention_2017,
  title    = {Attention {Is} {All} {You} {Need}},
  url      = {https://arxiv.org/abs/1706.03762v5},
  doi      = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  language = {en},
  urldate  = {2022-08-21},
  author   = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month    = jun,
  year     = {2017}
}

@inproceedings{webster_tokenization_1992,
  title  = {Tokenization as the initial phase in {NLP}},
  author = {Webster, Jonathan J and Kit, Chunyu},
  year   = {1992},
  file   = {Webster and Kit - 1992 - Tokenization as the initial phase in NLP.pdf:/home/avandebrook/Zotero/storage/CRJ234K6/Webster and Kit - 1992 - Tokenization as the initial phase in NLP.pdf:application/pdf}
}

@inproceedings{wolf_transformers_2020,
  address    = {Online},
  title      = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
  shorttitle = {Transformers},
  url        = {https://aclanthology.org/2020.emnlp-demos.6},
  doi        = {10.18653/v1/2020.emnlp-demos.6},
  abstract   = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  urldate    = {2023-11-30},
  booktitle  = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
  publisher  = {Association for Computational Linguistics},
  author     = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor     = {Liu, Qun and Schlangen, David},
  month      = oct,
  year       = {2020},
  pages      = {38--45},
  file       = {Full Text PDF:/home/avandebrook/Zotero/storage/AMMK9NK6/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf}
}

@misc{wu_googles_2016,
  title      = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
  shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
  url        = {http://arxiv.org/abs/1609.08144},
  doi        = {10.48550/arXiv.1609.08144},
  abstract   = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  month      = oct,
  year       = {2016},
  note       = {arXiv:1609.08144 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{xiao_speech_2022,
  title     = {Speech {Recognition} {Model} of {Civil} {Aviation} {Radiotelephony} {Communication} {Based} on {Improved} {Conformer}},
  doi       = {10.1109/ICARCE55724.2022.10046493},
  abstract  = {Radiotelephony communication has a special grammatical structure and pronunciation, and it is difficult to apply the model of generic speech recognition directly to the field of radiotelephony communication. We propose a Conv1DSlide-Conformer model for speech recognition of radiotelephony communication. The sliding-window attention mechanism is used instead of the self-attention mechanism to improve the decoding speed of the model and increase the adaptability of the model to radiotelephony communication. The convolutional module is used instead of the feedforward neural network module to make the encoder focus more on local information. The improved Conformer model processes the FBANK features of radiotelephony communication and can extract high-dimensional features that better fit the characteristics of radiotelephony communication. The use of concatenated temporal classification (CTC) combined with a data augmentation strategy assists training to speed up convergence during model training and reduce the complexity of model training. Decoding is assisted by CTC and language models to improve the performance of speech recognition. The experimental results show that the improved Conformer speech recognition model in this paper reduces the word error rate to 8.1\% and 7.8\% on the actual Chinese radiotelephony communication speech dataset.},
  booktitle = {2022 {International} {Conference} on {Automation}, {Robotics} and {Computer} {Engineering} ({ICARCE})},
  author    = {Xiao, Zewei and Jia, Guimin and Shi, Bo},
  month     = dec,
  year      = {2022},
  keywords  = {Computational modeling, Error analysis, Training, Speech recognition, Data models, Feature extraction, Adaptation models, ASR, attention mechanism, convolution module, end-to-end model, radiotelephony communication},
  pages     = {1--6},
  file      = {IEEE Xplore Abstract Record:/home/avandebrook/Zotero/storage/2F359S7J/10046493.html:text/html;IEEE Xplore Full Text PDF:/home/avandebrook/Zotero/storage/IC3SD5I7/Xiao et al. - 2022 - Speech Recognition Model of Civil Aviation Radiote.pdf:application/pdf}
}

@misc{yadlowsky_pretraining_2023,
  title     = {Pretraining {Data} {Mixtures} {Enable} {Narrow} {Model} {Selection} {Capabilities} in {Transformer} {Models}},
  url       = {http://arxiv.org/abs/2311.00871},
  doi       = {10.48550/arXiv.2311.00871},
  abstract  = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  urldate   = {2023-11-29},
  publisher = {arXiv},
  author    = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  month     = nov,
  year      = {2023},
  note      = {arXiv:2311.00871 [cs, stat]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{zhang_ai_2022,
  title     = {The {AI} {Index} 2022 {Annual} {Report}},
  url       = {http://arxiv.org/abs/2205.03468},
  doi       = {10.48550/arXiv.2205.03468},
  abstract  = {Welcome to the fifth edition of the AI Index Report! The latest edition includes data from a broad set of academic, private, and nonprofit organizations as well as more self-collected data and original analysis than any previous editions, including an expanded technical performance chapter, a new survey of robotics researchers around the world, data on global AI legislation records in 25 countries, and a new chapter with an in-depth analysis of technical AI ethics metrics. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.},
  urldate   = {2023-11-07},
  publisher = {arXiv},
  author    = {Zhang, Daniel and Maslej, Nestor and Brynjolfsson, Erik and Etchemendy, John and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Sellitto, Michael and Sakhaee, Ellie and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.03468 [cs]},
  keywords  = {Computer Science - Artificial Intelligence}
}

@misc{zuluaga-gomez_atco2_2023,
  title      = {{ATCO2} corpus: {A} {Large}-{Scale} {Dataset} for {Research} on {Automatic} {Speech} {Recognition} and {Natural} {Language} {Understanding} of {Air} {Traffic} {Control} {Communications}},
  shorttitle = {{ATCO2} corpus},
  url        = {http://arxiv.org/abs/2211.04054},
  doi        = {10.48550/arXiv.2211.04054},
  abstract   = {Personal assistants, automatic speech recognizers and dialogue understanding systems are becoming more critical in our interconnected digital world. A clear example is air traffic control (ATC) communications. ATC aims at guiding aircraft and controlling the airspace in a safe and optimal manner. These voice-based dialogues are carried between an air traffic controller (ATCO) and pilots via very-high frequency radio channels. In order to incorporate these novel technologies into ATC (low-resource domain), large-scale annotated datasets are required to develop the data-driven AI systems. Two examples are automatic speech recognition (ASR) and natural language understanding (NLU). In this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering research on the challenging ATC field, which has lagged behind due to lack of annotated data. The ATCO2 corpus covers 1) data collection and pre-processing, 2) pseudo-annotations of speech data, and 3) extraction of ATC-related named entities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set corpus contains 4 hours of ATC speech with manual transcripts and a subset with gold annotations for named-entity recognition (callsign, command, value). 2) The ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched with automatic transcripts from an in-domain speech recognizer, contextual information, speaker turn information, signal-to-noise ratio estimate and English language detection score per sample. Both available for purchase through ELDA at http://catalog.elra.info/en-us/repository/browse/ELRA-S0484. 3) The ATCO2-test-set-1h corpus is a one-hour subset from the original test set corpus, that we are offering for free at https://www.atco2.org/data. We expect the ATCO2 corpus will foster research on robust ASR and NLU not only in the field of ATC communications but also in the general research community.},
  urldate    = {2023-06-28},
  publisher  = {arXiv},
  author     = {Zuluaga-Gomez, Juan and Veselý, Karel and Szöke, Igor and Blatt, Alexander and Motlicek, Petr and Kocour, Martin and Rigault, Mickael and Choukri, Khalid and Prasad, Amrutha and Sarfjoo, Seyyed Saeed and Nigmatulina, Iuliia and Cevenini, Claudia and Kolčárek, Pavel and Tart, Allan and Černocký, Jan and Klakow, Dietrich},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2211.04054 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
  file       = {arXiv Fulltext PDF:/home/avandebrook/Zotero/storage/IE9QSAFV/Zuluaga-Gomez et al. - 2023 - ATCO2 corpus A Large-Scale Dataset for Research o.pdf:application/pdf;arXiv.org Snapshot:/home/avandebrook/Zotero/storage/EBU873UZ/2211.html:text/html}
}

@inproceedings{zuluaga-gomez_automatic_2020,
  title     = {Automatic Speech Recognition Benchmark for Air-Traffic  Communications},
  author    = {Juan, Zuluaga-Gomez. and Motlicek, Petr and Zhan, Qingran  and Braun, Rudolf and Vesely, Karel},
  publisher = {ISCA},
  journal   = {Proceedings of Interspeech 2020},
  pages     = {2297-2301},
  year      = {2020},
  abstract  = {Advances in Automatic Speech Recognition (ASR) over the  last decade opened new areas of speech-based automation  such as in Air-Traffic Control (ATC) environments.  Currently, voice communication and Controller Pilot Data  Link Communications are the only way of contact between  pilots and Air-Traffic Controllers (ATCo), where the former  is the most widely used and the latter is a non-speech  method mandatory for oceanic messages and limited for some  domestically issues. ASR systems on ATCo environments  inherit increasing complexity due to accents from  non-English speakers, cockpit noise, speaker-dependent  biases and small in-domain ATC databases for training. In  this paper, we review the last advances related to ASR on  ATCo communication. Then, we introduce CleanSky EC H2020  ATCO2, a project that aims to develop a platform to  collect, organize and automatically pre-process ATCo data  from air space. We apply transfer learning from  out-of-domain corpus coupled with adaptation on seven  command-related corpora. The acoustic modelling is based on  conventional TDNN-HMMs trained using lattice-free MMI  objective function. The developed ASR achieves relative  improvement in word error rates of 29% when using transfer  learning and an additional 36% when adapting the model with  seven command-related databases, these results obtained  from EC H2020 SESAR project MALORCA Vienna database.},
  url       = {http://infoscience.epfl.ch/record/284987},
  doi       = {https://doi.org/10.21437/Interspeech.2020-2173}
}

@misc{zuluaga-gomez_contextual_2021,
  title      = {Contextual {Semi}-{Supervised} {Learning}: {An} {Approach} {To} {Leverage} {Air}-{Surveillance} and {Untranscribed} {ATC} {Data} in {ASR} {Systems}},
  shorttitle = {Contextual {Semi}-{Supervised} {Learning}},
  url        = {http://arxiv.org/abs/2104.03643},
  doi        = {10.48550/arXiv.2104.03643},
  abstract   = {Air traffic management and specifically air-traffic control (ATC) rely mostly on voice communications between Air Traffic Controllers (ATCos) and pilots. In most cases, these voice communications follow a well-defined grammar that could be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign used to address an airplane is an essential part of all ATCo-pilot communications. We propose a two-steps approach to add contextual knowledge during semi-supervised training to reduce the ASR system error rates at recognizing the part of the utterance that contains the callsign. Initially, we represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the contextual knowledge is added by second-pass decoding (i.e. lattice re-scoring). Results show that `unseen domains' (e.g. data from airports not present in the supervised training data) are further aided by contextual SSL when compared to standalone SSL. For this task, we introduce the Callsign Word Error Rate (CA-WER) as an evaluation metric, which only assesses ASR performance of the spoken callsign in an utterance. We obtained a 32.1\% CA-WER relative improvement applying SSL with an additional 17.5\% CA-WER improvement by adding contextual knowledge during SSL on a challenging ATC-based test set gathered from LiveATC.},
  urldate    = {2022-07-19},
  publisher  = {arXiv},
  author     = {Zuluaga-Gomez, Juan and Nigmatulina, Iuliia and Prasad, Amrutha and Motlicek, Petr and Veselý, Karel and Kocour, Martin and Szöke, Igor},
  month      = aug,
  year       = {2021},
  note       = {arXiv:2104.03643 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition}
}
