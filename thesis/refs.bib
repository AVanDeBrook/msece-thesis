
@inproceedings{arnold_knowledge_2022,
  address   = {Hybrid: Seattle, Washington + Online},
  title     = {Knowledge extraction from aeronautical messages ({NOTAMs}) with self-supervised language models for aircraft pilots},
  url       = {https://aclanthology.org/2022.naacl-industry.22},
  doi       = {10.18653/v1/2022.naacl-industry.22},
  abstract  = {During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This self-supervised approach, where smaller amounts of labeled data are enough for task-specific fine-tuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models (by pilots, airlines or service providers), which is a first to the best of our knowledge.},
  urldate   = {2022-08-25},
  booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Track}},
  publisher = {Association for Computational Linguistics},
  author    = {Arnold, Alexandre and Ernez, Fares and Kobus, Catherine and Martin, Marion-Cécile},
  month     = jul,
  year      = {2022},
  pages     = {188--196}
}

@article{badrinath_automatic_2022,
  title    = {Automatic {Speech} {Recognition} for {Air} {Traffic} {Control} {Communications}},
  volume   = {2676},
  issn     = {0361-1981},
  url      = {https://doi.org/10.1177/03611981211036359},
  doi      = {10.1177/03611981211036359},
  abstract = {A significant fraction of communications between air traffic controllers and pilots is through speech, via radio channels. Automatic transcription of air traffic control (ATC) communications has the potential to improve system safety, operational performance, and conformance monitoring, and to enhance air traffic controller training. We present an automatic speech recognition model tailored to the ATC domain that can transcribe ATC voice to text. The transcribed text is used to extract operational information such as call-sign and runway number. The models are based on recent improvements in machine learning techniques for speech recognition and natural language processing. We evaluate the performance of the model on diverse datasets.},
  language = {en},
  number   = {1},
  urldate  = {2022-07-13},
  journal  = {Transportation Research Record},
  author   = {Badrinath, Sandeep and Balakrishnan, Hamsa},
  month    = jan,
  year     = {2022},
  note     = {Publisher: SAGE Publications Inc},
  keywords = {Read},
  pages    = {798--810}
}

@inproceedings{breunig_lof_2000,
  address    = {Dallas Texas USA},
  title      = {{LOF}: identifying density-based local outliers},
  isbn       = {978-1-58113-217-5},
  shorttitle = {{LOF}},
  url        = {https://dl.acm.org/doi/10.1145/342009.335388},
  doi        = {10.1145/342009.335388},
  language   = {en},
  urldate    = {2023-09-13},
  booktitle  = {Proceedings of the 2000 {ACM} {SIGMOD} international conference on {Management} of data},
  publisher  = {ACM},
  author     = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, Jörg},
  month      = may,
  year       = {2000},
  pages      = {93--104}
}

@misc{devlin_bert_2019,
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {http://arxiv.org/abs/1810.04805},
  doi        = {10.48550/arXiv.1810.04805},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2022-08-20},
  publisher  = {arXiv},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = may,
  year       = {2019},
  note       = {arXiv:1810.04805 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{falcon_pytorchlightning_2019,
  title   = {Pytorch lightning},
  volume  = {3},
  journal = {GitHub},
  author  = {Falcon, William A},
  year    = {2019}
}

@misc{gage_feb94_1994,
  title      = {{FEB94} {A} {New} {Algorithm} for {Data} {Compression}},
  shorttitle = {A {New} {Algorithm} for {Data} {Compression}},
  url        = {http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM},
  language   = {en},
  urldate    = {2023-09-29},
  author     = {Gage, Phillip},
  year       = {1994}
}

@misc{godfrey_air_1994,
  title     = {Air {Traffic} {Control} {Complete}},
  url       = {https://catalog.ldc.upenn.edu/LDC94S14A},
  abstract  = {The Air Traffic Control Corpus (ATC0) is comprised of recorded speech for use in supporting research and development activities in the area of robust speech recognition in domains similar to air traffic control (several speakers, noisy channels, relatively small vocabulary, constrained languaged, etc.) The audio data is composed of voice communication traffic between various controllers and pilots.},
  language  = {en},
  urldate   = {2022-07-16},
  publisher = {Linguistic Data Consortium},
  author    = {Godfrey, John J.},
  year      = {1994},
  keywords  = {Read}
}

@misc{guo_comparative_2022,
  title     = {A {Comparative} {Study} of {Speaker} {Role} {Identification} in {Air} {Traffic} {Communication} {Using} {Deep} {Learning} {Approaches}},
  url       = {http://arxiv.org/abs/2111.02041},
  doi       = {10.48550/arXiv.2111.02041},
  abstract  = {Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56\%, and 98.08\% accuracy, respectively.},
  urldate   = {2022-08-25},
  publisher = {arXiv},
  author    = {Guo, Dongyue and Zhang, Jianwei and Yang, Bo and Lin, Yi},
  month     = aug,
  year      = {2022},
  note      = {arXiv:2111.02041 [cs, eess]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{han_contextnet_2020,
  title      = {{ContextNet}: {Improving} {Convolutional} {Neural} {Networks} for {Automatic} {Speech} {Recognition} with {Global} {Context}},
  shorttitle = {{ContextNet}},
  url        = {http://arxiv.org/abs/2005.03191},
  doi        = {10.48550/arXiv.2005.03191},
  abstract   = {Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1\%/4.6\% without external language model (LM), 1.9\%/4.1\% with LM and 2.9\%/7.0\% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0\%/4.6\% with LM and 3.9\%/11.3\% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.},
  urldate    = {2022-07-13},
  publisher  = {arXiv},
  author     = {Han, Wei and Zhang, Zhengdong and Zhang, Yu and Yu, Jiahui and Chiu, Chung-Cheng and Qin, James and Gulati, Anmol and Pang, Ruoming and Wu, Yonghui},
  month      = may,
  year       = {2020},
  note       = {arXiv:2005.03191 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Read}
}

@article{helmke_quantifying_2017,
  title   = {Quantifying the benefits of speech recognition for an air traffic management application},
  journal = {Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2017},
  author  = {Helmke, Hartmut and Oualil, Youssef and Schulder, Marc},
  year    = {2017},
  note    = {Publisher: TUDpress, Dresden},
  pages   = {114--121}
}

@inproceedings{hofbauer_atcosim_2008,
  address   = {Marrakech, Morocco},
  title     = {The {ATCOSIM} {Corpus} of {Non}-{Prompted} {Clean} {Air} {Traffic} {Control} {Speech}},
  abstract  = {Air traffic control (ATC) is based on voice communication between pilots and controllers and uses a highly task and domain specific language. Due to this very reason, spoken language technologies for ATC require domain-specific corpora, of which only few exist to this day. The ATCOSIM Air Traffic Control Simulation Speech corpus is a speech database of non-prompted and clean ATC operator speech. It consists of ten hours of speech data, which were recorded in typical ATC control room conditions during ATC real-time simulations. The database includes orthographic transcriptions and additional information on speakers and recording sessions. The ATCOSIM corpus is publicly available and provided online free of charge. In this paper, we first give an overview of ATC related corpora and their shortcomings. We then show the difficulties in obtaining operational ATC speech recordings and propose the use of existing ATC real-time simulations. We describe the recording, transcription, production and validation process of the ATCOSIM corpus, and outline an application example for automatic speech recognition in the ATC domain.},
  urldate   = {2022-10-20},
  booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'08)},
  publisher = {European Language Resources Association (ELRA)},
  author    = {Hofbauer, Konrad and Petrik, Stefan and Hering, Horst},
  month     = may,
  year      = {2008}
}

@inproceedings{kriman_quartznet_2020,
  title      = {Quartznet: {Deep} {Automatic} {Speech} {Recognition} with {1D} {Time}-{Channel} {Separable} {Convolutions}},
  shorttitle = {Quartznet},
  doi        = {10.1109/ICASSP40776.2020.9053889},
  abstract   = {We propose a new end-to-end neural acoustic model for automatic speech recognition. The model is composed of multiple blocks with residual connections between them. Each block consists of one or more modules with 1D time-channel separable convolutional layers, batch normalization, and ReLU layers. It is trained with CTC loss. The proposed network achieves near state-of-the-art accuracy on LibriSpeech and Wall Street Journal, while having fewer parameters than all competing models. We also demonstrate that this model can be effectively fine-tuned on new datasets.},
  booktitle  = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author     = {Kriman, Samuel and Beliaev, Stanislav and Ginsburg, Boris and Huang, Jocelyn and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Zhang, Yang},
  month      = may,
  year       = {2020},
  note       = {ISSN: 2379-190X},
  keywords   = {Acoustics, Automatic speech recognition, Conferences, Convolution, convolutional networks, depthwise separable convolution, Speech processing, time-channel separable convolution, Read},
  pages      = {6124--6128}
}

@misc{kuchaiev_nemo_2019,
  title      = {{NeMo}: a toolkit for building {AI} applications using {Neural} {Modules}},
  shorttitle = {{NeMo}},
  url        = {http://arxiv.org/abs/1909.09577},
  doi        = {10.48550/arXiv.1909.09577},
  abstract   = {NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI applications through re-usability, abstraction, and composition. NeMo is built around neural modules, conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition and natural language processing. Furthermore, NeMo provides built-in support for distributed training and mixed precision on latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo},
  urldate    = {2023-12-01},
  publisher  = {arXiv},
  author     = {Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and Castonguay, Patrice and Popova, Mariya and Huang, Jocelyn and Cohen, Jonathan M.},
  month      = sep,
  year       = {2019},
  note       = {arXiv:1909.09577 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/home/avandebrook/Zotero/storage/WAPY54DL/Kuchaiev et al. - 2019 - NeMo a toolkit for building AI applications using.pdf:application/pdf;arXiv.org Snapshot:/home/avandebrook/Zotero/storage/BCU8WBBX/1909.html:text/html}
}

@misc{lewis_bart_2019,
  title      = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
  shorttitle = {{BART}},
  url        = {http://arxiv.org/abs/1910.13461},
  doi        = {10.48550/arXiv.1910.13461},
  abstract   = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  urldate    = {2022-08-21},
  publisher  = {arXiv},
  author     = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  month      = oct,
  year       = {2019},
  note       = {arXiv:1910.13461 [cs, stat]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  doi        = {10.48550/arXiv.1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2022-08-21},
  publisher  = {arXiv},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv:1907.11692 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{loshchilov_decoupled_2019,
  title     = {Decoupled {Weight} {Decay} {Regularization}},
  url       = {http://arxiv.org/abs/1711.05101},
  doi       = {10.48550/arXiv.1711.05101},
  abstract  = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  urldate   = {2023-11-24},
  publisher = {arXiv},
  author    = {Loshchilov, Ilya and Hutter, Frank},
  month     = jan,
  year      = {2019},
  note      = {arXiv:1711.05101 [cs, math]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control}
}

@inproceedings{maas_learning_2011,
  address   = {USA},
  series    = {{HLT} '11},
  title     = {Learning word vectors for sentiment analysis},
  isbn      = {978-1-932432-87-9},
  abstract  = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
  urldate   = {2022-07-16},
  booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Volume} 1},
  publisher = {Association for Computational Linguistics},
  author    = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  month     = jun,
  year      = {2011},
  pages     = {142--150}
}

@article{madeira_machine_2021,
  title     = {Machine {Learning} and {Natural} {Language} {Processing} for {Prediction} of {Human} {Factors} in {Aviation} {Incident} {Reports}},
  volume    = {8},
  copyright = {© 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  url       = {https://www.proquest.com/docview/2524211382/abstract/5B09B07DC1084F4EPQ/1},
  doi       = {10.3390/aerospace8020047},
  abstract  = {In the aviation sector, human factors are the primary cause of safety incidents. Intelligent prediction systems, which are capable of evaluating human state and managing risk, have been developed over the years to identify and prevent human factors. However, the lack of large useful labelled data has often been a drawback to the development of these systems. This study presents a methodology to identify and classify human factor categories from aviation incident reports. For feature extraction, a text pre-processing and Natural Language Processing (NLP) pipeline is developed. For data modelling, semi-supervised Label Spreading (LS) and supervised Support Vector Machine (SVM) techniques are considered. Random search and Bayesian optimization methods are applied for hyper-parameter analysis and the improvement of model performance, as measured by the Micro F1 score. The best predictive models achieved a Micro F1 score of 0.900, 0.779, and 0.875, for each level of the taxonomic framework, respectively. The results of the proposed method indicate that favourable predicting performances can be achieved for the classification of human factors based on text data. Notwithstanding, a larger data set would be recommended in future research.},
  language  = {English},
  number    = {2},
  urldate   = {2022-08-25},
  journal   = {Aerospace},
  author    = {Madeira, Tomás and Melício, Rui and Valério, Duarte and Santos, Luis},
  year      = {2021},
  note      = {Num Pages: 47
               Place: Basel, Switzerland
               Publisher: MDPI AG},
  keywords  = {aviation incident reports, aviation safety, human factors, machine learning, natural language processing, prediction},
  pages     = {47}
}

@misc{majumdar_citrinet_2021,
  title      = {Citrinet: {Closing} the {Gap} between {Non}-{Autoregressive} and {Autoregressive} {End}-to-{End} {Models} for {Automatic} {Speech} {Recognition}},
  shorttitle = {Citrinet},
  url        = {http://arxiv.org/abs/2104.01721},
  doi        = {10.48550/arXiv.2104.01721},
  abstract   = {We propose Citrinet - a new end-to-end convolutional Connectionist Temporal Classification (CTC) based automatic speech recognition (ASR) model. Citrinet is deep residual neural model which uses 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation. The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models. We evaluate Citrinet on LibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English speech datasets. Citrinet accuracy on these datasets is close to the best autoregressive Transducer models.},
  urldate    = {2023-11-21},
  publisher  = {arXiv},
  author     = {Majumdar, Somshubra and Balam, Jagadeesh and Hrinchuk, Oleksii and Lavrukhin, Vitaly and Noroozi, Vahid and Ginsburg, Boris},
  month      = apr,
  year       = {2021},
  note       = {arXiv:2104.01721 [eess]},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{merity_pointer_2016,
  title    = {Pointer {Sentinel} {Mixture} {Models}},
  url      = {https://arxiv.org/abs/1609.07843v1},
  abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
  language = {en},
  urldate  = {2023-10-20},
  journal  = {arXiv.org},
  author   = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  month    = sep,
  year     = {2016}
}

@book{paltridge_handbook_2013,
  address   = {Chichester, West Sussex},
  title     = {The {Handbook} of {English} for {Specific} {Purposes}},
  isbn      = {978-1-118-29058-3 978-1-118-29059-0 978-1-118-29060-6},
  language  = {en},
  publisher = {Wiley-Blackwell},
  editor    = {Paltridge, Brian and Starfield, Sue},
  year      = {2013},
  keywords  = {Anthropological ethics, Handbooks, manuals, etc, SOCIAL SCIENCE / Anthropology / Cultural}
}

@inproceedings{paszke_pytorch_2019,
  title      = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
  volume     = {32},
  shorttitle = {{PyTorch}},
  abstract   = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
                In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
                We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  urldate    = {2023-11-30},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year       = {2019}
}

@article{pedregosa_scikit-learn_2011,
  title   = {Scikit-learn: {Machine} learning in {Python}},
  volume  = {12},
  issn    = {1532-4435},
  journal = {the Journal of machine Learning research},
  author  = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
  year    = {2011},
  note    = {Publisher: JMLR. org},
  pages   = {2825--2830},
  file    = {Pedregosa et al. - 2011 - Scikit-learn Machine learning in Python.pdf:/home/avandebrook/Zotero/storage/9XDDCW86/Pedregosa et al. - 2011 - Scikit-learn Machine learning in Python.pdf:application/pdf}
}

@inproceedings{pellegrini_airbus_2019,
  title      = {The {Airbus} {Air} {Traffic} {Control} speech recognition 2018 challenge: towards {ATC} automatic transcription and call sign detection},
  shorttitle = {The {Airbus} {Air} {Traffic} {Control} speech recognition 2018 challenge},
  url        = {http://arxiv.org/abs/1810.12614},
  doi        = {10.21437/Interspeech.2019-1962},
  abstract   = {In this paper, we describe the outcomes of the challenge organized and run by Airbus and partners in 2018. The challenge consisted of two tasks applied to Air Traffic Control (ATC) speech in English: 1) automatic speech-to-text transcription, 2) call sign detection (CSD). The registered participants were provided with 40 hours of speech along with manual transcriptions. Twenty-two teams submitted predictions on a five hour evaluation set. ATC speech processing is challenging for several reasons: high speech rate, foreign-accented speech with a great diversity of accents, noisy communication channels. The best ranked team achieved a 7.62\% Word Error Rate and a 82.41\% CSD F1-score. Transcribing pilots' speech was found to be twice as harder as controllers' speech. Remaining issues towards solving ATC ASR are also discussed.},
  urldate    = {2023-04-06},
  booktitle  = {Interspeech 2019},
  author     = {Pellegrini, Thomas and Farinas, Jérôme and Delpech, Estelle and Lancelot, François},
  month      = sep,
  year       = {2019},
  note       = {arXiv:1810.12614 [cs, eess]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  pages      = {2993--2997}
}

@article{radford_improving_2018,
  title  = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year   = {2018},
  note   = {Publisher: OpenAI}
}

@article{radford_language_2019,
  title   = {Language models are unsupervised multitask learners},
  volume  = {1},
  number  = {8},
  journal = {OpenAI blog},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year    = {2019},
  pages   = {9}
}

@misc{raffel_exploring_2020,
  title     = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
  url       = {http://arxiv.org/abs/1910.10683},
  doi       = {10.48550/arXiv.1910.10683},
  abstract  = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  urldate   = {2022-08-21},
  publisher = {arXiv},
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  month     = jul,
  year      = {2020},
  note      = {arXiv:1910.10683 [cs, stat]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{ragnarsdottir_language_2003,
  title     = {Language technology in air traffic control},
  volume    = {1},
  doi       = {10.1109/DASC.2003.1245815},
  abstract  = {Voice communication is a volatile part of Air Traffic Control (ATC). According to research, on average one miscommunication happens every hour per radio frequency where there is frequent communication such as in TRACON. ICAO puts great emphasis on improving communication in ATC. This article proposes that a language technology system (LTS) can make communication between controller and pilot more reliable and efficient, thus improving safety in aviation. An LTS can for example detect readback errors. It can also directly feed data from the voice recognizer on XML (eXtensible Markup Language) form into a flight data processing system or interact with it as we show. By interviewing air traffic controllers and studying the literature, we identified several examples of use of language technology in ATC. As an example we take a system to support controllers in their work by making the LTS give warnings when discrepancy is found in the communications between controller and pilot. This system is not meant to control the airspace autonomously. Latest advances in language technology have enabled the development of such a system. The functionality of the proposed LTS is described using scenarios and sequence diagrams. A demonstration conversational agent using Hex Technology was implemented. A Wizard of Oz usability test was administered to seven controllers. Their attitude to the agent was positive and indicates that there is reason for further research. The performance and error logs of the agent and voice server were analyzed and give guidance on further development of a fully functioning language technology system for air traffic control.},
  booktitle = {Digital {Avionics} {Systems} {Conference}, 2003. {DASC} '03. {The} 22nd},
  author    = {Ragnarsdottir, M.D. and Waage, H. and Hvannberg, E.T.},
  month     = oct,
  year      = {2003},
  keywords  = {Speech recognition, Air traffic control, Aircraft, Grammar, Natural language processing, Safety, Speech},
  pages     = {2.E.2--21--13 vol.1}
}

@inproceedings{schuster_japanese_2012,
  title     = {Japanese and {Korean} voice search},
  url       = {https://ieeexplore.ieee.org/document/6289079},
  doi       = {10.1109/ICASSP.2012.6289079},
  abstract  = {This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.},
  urldate   = {2023-11-16},
  booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  month     = mar,
  year      = {2012},
  note      = {ISSN: 2379-190X},
  pages     = {5149--5152}
}

@misc{sennrich_neural_2016,
  title     = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
  url       = {http://arxiv.org/abs/1508.07909},
  doi       = {10.48550/arXiv.1508.07909},
  abstract  = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  urldate   = {2022-07-13},
  publisher = {arXiv},
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1508.07909 [cs]},
  keywords  = {Computer Science - Computation and Language, Unread}
}

@article{smidl_air_2019,
  title    = {Air traffic control communication ({ATCC}) speech corpora and their use for {ASR} and {TTS} development},
  volume   = {53},
  issn     = {1574-0218},
  url      = {https://doi.org/10.1007/s10579-019-09449-5},
  doi      = {10.1007/s10579-019-09449-5},
  abstract = {The paper introduces the motivation for creating dedicated speech corpora of air traffic control communication, describes in detail the process of preparation of corpora for both automatic speech recognition and text-to-speech synthesis, presents an illustrative example of speech recognition system developed using the automatic speech recognition corpora and finally describes the technical aspects of the data and the distribution channel.},
  language = {en},
  number   = {3},
  urldate  = {2022-07-14},
  journal  = {Language Resources and Evaluation},
  author   = {Šmídl, Luboš and Švec, Jan and Tihelka, Daniel and Matoušek, Jindřich and Romportl, Jan and Ircing, Pavel},
  month    = sep,
  year     = {2019},
  keywords = {Air traffic control communication, Read, Speech corpus, Speech recognition, Text-to-speech},
  pages    = {449--464}
}

@book{srinivasamurthy_semi-supervised_2017,
  title    = {Semi-supervised {Learning} with {Semantic} {Knowledge} {Extraction} for {Improved} {Speech} {Recognition} in {Air} {Traffic} {Control}},
  abstract = {Automatic Speech Recognition (ASR) can introduce higher levels of automation into Air Traffic Control (ATC), where spoken language is still the predominant form of communication. While ATC uses standard phraseology and a limited vocabulary, we need to adapt the speech recognition systems to local acoustic conditions and vocabularies at each airport to reach optimal performance. Due to continuous operation of ATC systems, a large and increasing amount of untranscribed speech data is available, allowing for semi-supervised learning methods to build and adapt ASR models. In this paper, we first identify the challenges in building ASR systems for specific ATC areas and propose to utilize out-of-domain data to build baseline ASR models. Then we explore different methods of data selection for adapting baseline models by exploiting the continuously increasing untranscribed data. We develop a basic approach capable of exploiting semantic representations of ATC commands. We achieve relative improvement in both word error rate (23.5\%) and concept error rates (7\%) when adapting ASR models to different ATC conditions in a semi-supervised manner},
  editor   = {Srinivasamurthy, Ajay and Motlicek, Petr and Himawan, Ivan and Szaszak, Gyorgy and Oualil, Youssef and Helmke, Hartmut},
  year     = {2017},
  doi      = {10.21437/Interspeech.2017-1446},
  note     = {Meeting Name: Proceedings of Interspeech 2017}
}

@article{szoke_detecting_2021,
  title    = {Detecting {English} {Speech} in the {Air} {Traffic} {Control} {Voice} {Communication}},
  url      = {https://arxiv.org/abs/2104.02332v1},
  doi      = {10.48550/arXiv.2104.02332},
  abstract = {We launched a community platform for collecting the ATC speech world-wide in the ATCO2 project. Filtering out unseen non-English speech is one of the main components in the data processing pipeline. The proposed English Language Detection (ELD) system is based on the embeddings from Bayesian subspace multinomial model. It is trained on the word confusion network from an ASR system. It is robust, easy to train, and light weighted. We achieved 0.0439 equal-error-rate (EER), a 50\% relative reduction as compared to the state-of-the-art acoustic ELD system based on x-vectors, in the in-domain scenario. Further, we achieved an EER of 0.1352, a 33\% relative reduction as compared to the acoustic ELD, in the unseen language (out-of-domain) condition. We plan to publish the evaluation dataset from the ATCO2 project.},
  language = {en},
  urldate  = {2022-11-05},
  author   = {Szoke, Igor and Kesiraju, Santosh and Novotny, Ondrej and Kocour, Martin and Vesely, Karel and Cernocky, Jan "Honza"},
  month    = apr,
  year     = {2021}
}

@article{tanguy_natural_2016,
  series     = {Natural {Language} {Processing} and {Text} {Analytics} in {Industry}},
  title      = {Natural language processing for aviation safety reports: {From} classification to interactive analysis},
  volume     = {78},
  issn       = {0166-3615},
  shorttitle = {Natural language processing for aviation safety reports},
  url        = {https://www.sciencedirect.com/science/article/pii/S0166361515300464},
  doi        = {10.1016/j.compind.2015.09.005},
  abstract   = {In this paper we describe the different NLP techniques designed and used in collaboration between the CLLE-ERSS research laboratory and the CFH/Safety Data company to manage and analyse aviation incident reports. These reports are written every time anything abnormal occurs during a civil air flight. Although most of them relate routine problems, they are a valuable source of information about possible sources of greater danger. These texts are written in plain language, show a wide range of linguistic variation (telegraphic style overcrowded by acronyms or standard prose) and exist in different languages, even for a single company/country (although our main focus is on English and French). In addition to their variety, their sheer quantity (e.g. 600/month for a large airline company) clearly requires the use of advanced NLP and text mining techniques in order to extract useful information from them. Although this context and objectives seem to indicate that standard NLP techniques can be applied in a straightforward manner, innovative techniques are required to handle the specifics of aviation report text and the complex classification systems. We present several tools that aim at a better access to this data (classification and information retrieval), and help aviation safety experts in their analyses (data/text mining and interactive analysis). Some of these tools are currently in test or in use both at the national and international levels, by airline companies as well as by regulation authorities (DGAC,11Direction Générale de l’Aviation Civile. EASA,22European Aviation Safety Agency. ICAO33International Civil Aviation Organization.).},
  language   = {en},
  urldate    = {2022-08-25},
  journal    = {Computers in Industry},
  author     = {Tanguy, Ludovic and Tulechki, Nikola and Urieli, Assaf and Hermann, Eric and Raynal, Céline},
  month      = may,
  year       = {2016},
  keywords   = {Aviation, Document classification, NLP, Safety reports, Text mining},
  pages      = {80--95}
}

@article{vaswani_attention_2017,
  title    = {Attention {Is} {All} {You} {Need}},
  url      = {https://arxiv.org/abs/1706.03762v5},
  doi      = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  language = {en},
  urldate  = {2022-08-21},
  author   = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month    = jun,
  year     = {2017}
}

@inproceedings{wolf_transformers_2020,
  address    = {Online},
  title      = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
  shorttitle = {Transformers},
  url        = {https://aclanthology.org/2020.emnlp-demos.6},
  doi        = {10.18653/v1/2020.emnlp-demos.6},
  abstract   = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  urldate    = {2023-11-30},
  booktitle  = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
  publisher  = {Association for Computational Linguistics},
  author     = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor     = {Liu, Qun and Schlangen, David},
  month      = oct,
  year       = {2020},
  pages      = {38--45},
  file       = {Full Text PDF:/home/avandebrook/Zotero/storage/AMMK9NK6/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf}
}

@misc{wu_googles_2016,
  title      = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
  shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
  url        = {http://arxiv.org/abs/1609.08144},
  doi        = {10.48550/arXiv.1609.08144},
  abstract   = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  month      = oct,
  year       = {2016},
  note       = {arXiv:1609.08144 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{yadlowsky_pretraining_2023,
  title     = {Pretraining {Data} {Mixtures} {Enable} {Narrow} {Model} {Selection} {Capabilities} in {Transformer} {Models}},
  url       = {http://arxiv.org/abs/2311.00871},
  doi       = {10.48550/arXiv.2311.00871},
  abstract  = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  urldate   = {2023-11-29},
  publisher = {arXiv},
  author    = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  month     = nov,
  year      = {2023},
  note      = {arXiv:2311.00871 [cs, stat]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{zhang_ai_2022,
  title     = {The {AI} {Index} 2022 {Annual} {Report}},
  url       = {http://arxiv.org/abs/2205.03468},
  doi       = {10.48550/arXiv.2205.03468},
  abstract  = {Welcome to the fifth edition of the AI Index Report! The latest edition includes data from a broad set of academic, private, and nonprofit organizations as well as more self-collected data and original analysis than any previous editions, including an expanded technical performance chapter, a new survey of robotics researchers around the world, data on global AI legislation records in 25 countries, and a new chapter with an in-depth analysis of technical AI ethics metrics. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.},
  urldate   = {2023-11-07},
  publisher = {arXiv},
  author    = {Zhang, Daniel and Maslej, Nestor and Brynjolfsson, Erik and Etchemendy, John and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Sellitto, Michael and Sakhaee, Ellie and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.03468 [cs]},
  keywords  = {Computer Science - Artificial Intelligence}
}

@misc{zuluaga-gomez_automatic_2020,
  title     = {Automatic {Speech} {Recognition} {Benchmark} for {Air}-{Traffic} {Communications}},
  url       = {http://arxiv.org/abs/2006.10304},
  doi       = {10.48550/arXiv.2006.10304},
  abstract  = {Advances in Automatic Speech Recognition (ASR) over the last decade opened new areas of speech-based automation such as in Air-Traffic Control (ATC) environment. Currently, voice communication and data links communications are the only way of contact between pilots and Air-Traffic Controllers (ATCo), where the former is the most widely used and the latter is a non-spoken method mandatory for oceanic messages and limited for some domestic issues. ASR systems on ATCo environments inherit increasing complexity due to accents from non-English speakers, cockpit noise, speaker-dependent biases, and small in-domain ATC databases for training. Hereby, we introduce CleanSky EC-H2020 ATCO2, a project that aims to develop an ASR-based platform to collect, organize and automatically pre-process ATCo speech-data from air space. This paper conveys an exploratory benchmark of several state-of-the-art ASR models trained on more than 170 hours of ATCo speech-data. We demonstrate that the cross-accent flaws due to speakers' accents are minimized due to the amount of data, making the system feasible for ATC environments. The developed ASR system achieves an averaged word error rate (WER) of 7.75\% across four databases. An additional 35\% relative improvement in WER is achieved on one test set when training a TDNNF system with byte-pair encoding.},
  urldate   = {2022-07-17},
  publisher = {arXiv},
  author    = {Zuluaga-Gomez, Juan and Motlicek, Petr and Zhan, Qingran and Vesely, Karel and Braun, Rudolf},
  month     = aug,
  year      = {2020},
  note      = {arXiv:2006.10304 [cs, eess]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition, Unread}
}

@misc{zuluaga-gomez_contextual_2021,
  title      = {Contextual {Semi}-{Supervised} {Learning}: {An} {Approach} {To} {Leverage} {Air}-{Surveillance} and {Untranscribed} {ATC} {Data} in {ASR} {Systems}},
  shorttitle = {Contextual {Semi}-{Supervised} {Learning}},
  url        = {http://arxiv.org/abs/2104.03643},
  doi        = {10.48550/arXiv.2104.03643},
  abstract   = {Air traffic management and specifically air-traffic control (ATC) rely mostly on voice communications between Air Traffic Controllers (ATCos) and pilots. In most cases, these voice communications follow a well-defined grammar that could be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign used to address an airplane is an essential part of all ATCo-pilot communications. We propose a two-steps approach to add contextual knowledge during semi-supervised training to reduce the ASR system error rates at recognizing the part of the utterance that contains the callsign. Initially, we represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the contextual knowledge is added by second-pass decoding (i.e. lattice re-scoring). Results show that `unseen domains' (e.g. data from airports not present in the supervised training data) are further aided by contextual SSL when compared to standalone SSL. For this task, we introduce the Callsign Word Error Rate (CA-WER) as an evaluation metric, which only assesses ASR performance of the spoken callsign in an utterance. We obtained a 32.1\% CA-WER relative improvement applying SSL with an additional 17.5\% CA-WER improvement by adding contextual knowledge during SSL on a challenging ATC-based test set gathered from LiveATC.},
  urldate    = {2022-07-19},
  publisher  = {arXiv},
  author     = {Zuluaga-Gomez, Juan and Nigmatulina, Iuliia and Prasad, Amrutha and Motlicek, Petr and Veselý, Karel and Kocour, Martin and Szöke, Igor},
  month      = aug,
  year       = {2021},
  note       = {arXiv:2104.03643 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition}
}
