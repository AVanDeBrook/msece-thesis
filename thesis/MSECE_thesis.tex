\documentclass[12pt]{article}

\usepackage{cite}
\usepackage{listings}
\usepackage{multicol}
\usepackage{newtxtext}
\usepackage{setspace}
\usepackage{sectsty}
\usepackage{booktabs}
\usepackage[numbib]{tocbibind}
\usepackage[lmargin=1.25in,rmargin=1.25in,tmargin=1.0in,bmargin=1.0in]{geometry}

% \settowidth{\parindent}{}

\begin{document}
% Riddle thesis title page
\begin{center}
    \LARGE
    \textbf{
        \textit{
            Language Modeling of Aviation Communications\\
        }
    }

    \vspace*{0.2in}

    \normalsize
    by\\
    \textit{Aaron P.~Van De Brook}\\

    \vspace*{0.25in}

\end{center}
This thesis was prepared under the direction of the candidate's thesis
committee chairman, \underline{Dr.~Jianhua Liu}, Department of \underline{Electrical Engineering \&
    Computer Science}, and has been approved by the members of the thesis committee.
It was submitted to the Department of Electrical Engineering \& Computer Science
and was accepted in partial fulfillment of the requirements for the degree of
Masters of Electrical \& Computer Engineering.

% Signature lines/committee members
\begin{center}
    \begin{minipage}{3in}
        \vspace*{0.25in}
        THESIS COMMITTEE:\\
        \vspace*{0.25in}
        \hrule
        \vspace*{2pt}
        Jianhua Liu, Ph.D.\\
        Committee Chairman\\
        \vspace*{0.75in}
        \hrule
        \vspace*{2pt}
        Prashant Shekhar, Ph.D.\\
        Committee Member\\
        \vspace*{0.75in}
        \hrule
        \vspace*{2pt}
        Andrew Schneider, Ph.D.\\
        Committee Member\\
        \vspace*{0.75in}
        \hrule
        \vspace*{2pt}
        Farahzad Behi\\
        Department Chair, Electrical Engineering \& Computer Science\\
        \vspace*{0.75in}
        \hrule
        \vspace*{2pt}
        Associate Vice President for Academics
    \end{minipage}
\end{center}
\newpage
\tableofcontents
\newpage
\doublespacing{}

% \section{Abstract}

\section{Literature Review}
% from proposal; revise and add to this as more references are used
% transformer history
The transformer neural network architecture was proposed in 2017 for Neural Machine Translation tasks and immediately achieved
state-of-the-art (28.4 BLEU on WMT 2014 English-to-German; 41.8 BLEU on WMT English-to-French datasets)\cite{vaswani_attention_2017}. Transformer
architectures have been found to be extremely effective at learning representations and understandings of languages to predict token probabilities as
opposed to transforming one language into another\cite{devlin_bert_2019,liu_roberta_2019}. Transformers have also been found to be very effective at
other NLP-related tasks such as prompt completion and sentiment analysis among others (i.e.~auto-regressive and sequence classification tasks,
respectively)\cite{lewis_bart_2019,radford_improving_2018}.


% ASR context (suggest some need for semi-supervised approaches and therefore LMs)
End-to-end generative models for automatic speech recognition models have made significant progress in recent years with current state-of-the-art
models achieving WERs as low as 2\% on LibriSpeech test sets\cite{han_contextnet_2020,kriman_quartznet_2020,baevski_wav2vec_2020,li_jasper_2019}.
This has led to the development of ASR models for the aviation domain, specifically, in air traffic control communications\cite{badrinath_automatic_2022,smidl_air_2019,zuluaga-gomez_automatic_2020,srinivasamurthy_semi-supervised_2017}. However, due to the lack of
transcribed data in the aviation domain\cite{zuluaga-gomez_automatic_2020,srinivasamurthy_semi-supervised_2017,badrinath_automatic_2022,smidl_air_2019}
ASR models maintain relatively high WERs compared to their counterparts in the more generalized ASR domain\cite{zuluaga-gomez_automatic_2020,badrinath_automatic_2022}. Transfer learning has even yielded limited results in this domain (depending on model
architecture and dataset quality)\cite{badrinath_automatic_2022,zuluaga-gomez_automatic_2020}.


% Unsupervised/semi-supervised in general and LMs commonly used with them
Unsupervised and semi-supervised methodologies have become popular recently in attempts to address limited data availability
and develop new approaches towards modeling human speech (notably, wav2vec has achieved state-of-the-art performance with
very little training data)\cite{baevski_wav2vec_2020,badrinath_automatic_2022,srinivasamurthy_semi-supervised_2017,zuluaga-gomez_contextual_2021}.
Language models are an integral part semi-supervised learning. They are used to obtain a certainty score (or uncertainty
score, as the case may be) for the predicted text, these are usually either word lattices or N-gram models\cite{badrinath_automatic_2022,srinivasamurthy_semi-supervised_2017,zuluaga-gomez_contextual_2021}.
While these have proven to be adequate for most self-supervised learning tasks, they are hardly state-of-the-art.
Wav2vec, the current state-of-the-art unsupervised model (and possibly top performing overall), uses a contrastive process
between convolutional feature extraction and transformer context representations\cite{baevski_wav2vec_2020}.


% LMs and NLP in aero. domain
Various natural language processing methods have been applied to the aviation domain to help deal with miscommunications
and try to mitigate safety incidents\cite{ragnarsdottir_language_2003,tanguy_natural_2016,madeira_machine_2021}.
Some machine learning approaches have been implemented to analyze the text in aviation incident and safety reports to predict
contributing factors and topic models\cite{tanguy_natural_2016,madeira_machine_2021}. An ASR system with NLP post-processing
has also been proposed to analyze Air-Traffic communications and condense significant features (e.g.~weather, runway, and
heading info) into an XML language structure\cite{ragnarsdottir_language_2003}. Transformer language models such as BERT,
RoBERTa, and DeBERTa have been applied to notice to airmen (NOTAM) messages to perform named entity recognition (NER) tasks,
translation tasks (between notations; e.g.~NOTAM notation to Airlang to make parsing tasks easier) and reduce the workload
for pilots during long-haul flights\cite{arnold_knowledge_2022}. Transformer models have also been used for speaker role
identification tasks in the aviation domain (e.g.~identifying pilot versus controller in communications), specifically,
a pretrained BERT model was used and fine-tuned on problem specific data and compared to other models that performed
well at speaker and role identification tasks in general\cite{guo_comparative_2022}.

\section{Data Source(s)}\label{sec:data_source}
There are four datasets used in this work. The datasets are primarily speech corpora intended either for automatic speech recognition
research in the domain of aviation communications, linguistics research in aviation english, or both. As a result, the text-based data
samples that are used to train the language models in this work are transcripts of aural speech. The datasets used are described in more
detail below. All recordings and transcripts are in english, although there are multiple instances of foreign words occuring

\subsection{Air Traffic Control Complete}
The Air Traffic Control Complete (ATCC) corpus is a speech corpus consisting of audio recordings with corresponding transcriptions.
ATCC is a collection of three smaller corpora recorded at Dallas-Fort Worth, Logan International, and Washington National airports and
transcribed by subject matter experts (usually controllers and/or pilots) familiar with the terminology specific to their respective
areas\cite{godfrey_air_1994}.

\subsection{ATCO2}
The ATCO2 dataset is a speech corpus that consists of audio communications at Prague and Brno airports, in Czechia, along with corresponding
transcriptions. The speech recordings and transcripts are crowd sourced from volunteers\cite{szoke_detecting_2021}.


\subsection{Air Traffic Control Simulation}
The Air Traffic Control Simulation (ATCOSIM) corpus is a speech corpus consisting of audio recordings and corresponding transcriptions.
The data was recorded at a air traffic control simulation center in Bretigny-sur-Orge, France, specifically the EUROCONTROL Experimental Centre.
In this data, only the controllers' voices are included and therefore the transcripts only include the controller's side of each interaction.
The audio data was transcribed by one person, trained according to the guidelines and formatting requirements of the corpus. After all data was
transcribed, it was reviewed and corrected and any remaining problems were reviewed by an operational air traffic controller and
resolved\cite{hofbauer_atcosim_2008}.

\subsection{ZCU CZ ATC Corpus}
The ZCU CZ ATC corpus consists of audio recordings and corresponding transcripts in the Czech airspace. Both controller and pilot sides of the
communications are included in this data. Annotations were created by experienced transcribers/annotators and labeled samples were randomly selected
for review during the annoation process. After the dataset was completely labeled, all samples were checked, corrected, and
unified\cite{smidl_air_2019}.

\section{Data Analysis}
% sequence lengths, unique tokens, total number of tokens, total number of samples, breakdown by each dataset
\begin{table}
    \centering
    \begin{tabular}{l r r r r}
        Corpus                         & Samples & Mean Sequence Length & Unique Tokens & Tokens  \\
        \toprule
        Air Traffic Control Complete   & 29,862  & 11.90                & 2,210         & 355,476 \\
        ATCO2                          & 874     & 12.46                & 855           & 10,886  \\
        Air Traffic Control Simulation & 9,544   & 11.29                & 824           & 107,759 \\
        ZCU CZ ATC Corpus              & 14,435  & 10.32                & 3,106         & 149,031 \\
        \bottomrule
        Total                          & 54,715  & 11.39                & 5,343         & 623,152 \\
    \end{tabular}
    \caption{Data statistics by corpus, including statistics across all corpora.}
\end{table}

\section{Data Processing}
All of the corpora listed above (see~\ref{sec:data_source}) are created at different times, for different purposes, and by different authors.
As a result, the data in these corpora all have different formats and need to be processed into a common format to be used together, with the goal
being to essentially create a super-corpus that includes all the corpora listed above.

All data was processed in Python using primarily built-in functions/modules\footnote{The \lstinline|re| and \lstinline|xml| modules were also used although
    these are built into most Python distributions by default.}. The text data is extracted from the corpus transcripts i.e.~Lisp in ATCC, XML in
ATCO2 and ZCU CZ ATC, and plain text in ATCOSIM, the transcriber annotations are removed where necessary\footnote{A copy of the original data is
    made, then the annotations are removed from the copy.}, and the resulting text is written in the standard corpus format (an ASCII text file with
one line in the file corresponding to one sample from the corpus) to a file.

\subsection{Data Preprocessing}
% explain and justify what is removed from the text, how, and why
% present and explain any relevant algorithms where necessary


\subsection{Tokenization}
% present and explain the tokenization algorithms used (possibly which models they correspond to)

\newpage
\bibliography{refs}
\bibliographystyle{plain}

\end{document}
