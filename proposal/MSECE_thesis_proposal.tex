\documentclass{article}

\usepackage{cite}
\usepackage{url}
\usepackage[numbib]{tocbibind}
\usepackage[lmargin=1in,rmargin=1in,tmargin=1in,bmargin=1in]{geometry}

\title{
    Master of Science in Electrical \& Computer Engineering\\
    \underline{Master's Thesis Proposal}\\
    Transformer-based Language Modeling of Air Traffic Communications
}

\author{Aaron Van De Brook---2456908}

\date{\today}

\begin{document}
    \maketitle

    % TODO: need to write abstract (probably last step)
    \begin{center}
        \section*{\textit{Abstract}}
        \textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.}
    \end{center}

    \section{Introduction}
        The Transformer neural network architecture was originally designed and created for Neural Machine Translation (NMT) tasks \cite{vaswani_attention_2017} and has since seen massive success in various other Natural Language Processing (NLP) tasks such as language modeling (both casual and masked language modeling tasks) \cite{devlin_bert_2019}, prompt completion \cite{radford_improving_2018}, and sequence-to-sequence (S2S) classification \cite{lewis_bart_2019}.
        Recently, Automatic Speech Recognition (ASR) models have started to incorporate language models \cite{badrinath_automatic_2022} (sometimes also referred to as linguistic models) in addition to acoustic models \cite{li2019jasper} as the state of the art models begin to shift towards generative end-to-end approaches \cite{hannun2014deep}.
        At the time of writing this, the current state of the art ASR models are a mix between Transformer and Convolutional neural network (CNN) architectures\footnote{According to \url{https://paperswithcode.com/sota/automatic-speech-recognition-on-lrs2}} \cite{baevski2020wav2vec}.


        As ASR models begin to breakout into the aerospace domain, one of the most limiting factors in their development and deployment has been their relatively high word error rate(s) (WER) \cite{vsmidl2019air,juan2020automatic,badrinath_automatic_2022} as compared to typical state of the art models.
        This is due, at least in part, to the severe lack of transcribed data as compared to the more general ASR domain.
        For instance the best estimates of combined labeled datasets for Air Traffic Control (ATC) communications reach approximately 180 hours of speech data \cite{juan2020automatic} whereas state of the art ASR datasets typically reach approximately 1000 hours of speech data \cite{librispeech_2015}.
        This has lead to the recommendation of semi-supervised learning models wherein unlabeled air traffic communications are transcribed by a pretrained ASR model with a relatively low WER.
        The transcriptions are subsequently scored by a language model to obtain some insight into the transcription quality \cite{badrinath_automatic_2022,zuluaga2021contextual}.
        Depending on the distribution of the transcription scores, a certain percentage (best $n$ candidates) of the transcribed data is used to further fine-tune the ASR model.


        Despite the effectiveness of transformer-based architectures for language modeling NLP tasks \cite{devlin_bert_2019,lewis_bart_2019,liu2019roberta}, there is little to no implementation of them for modeling ATC communications for semi-supervised learning or otherwise in the existing literature.
        Additionally, my preliminary work into the transfer learning of BERT models in the ATC domain yield surprisingly high perplexity scores before overfitting to the training data.
        This suggests that a more sophisticated approach is needed to make use of transformer-based architectures such as BERT or RoBERTa for modeling air traffic communications.

    \section{Literature Review}

    \section{Methodology}

    \section{Deliverables \& Timeline}
        \begin{table}[h!]
            \centering
            \begin{tabular}{|c|c|c|}
                \hline
                \# & Deliverable & Date\\
                \hline
                0 & Submit thesis proposal & \today\\
                1 & & \\
                \hline
            \end{tabular}
            \label{table:timeline}
            \caption{Deliverables and approximate delivery dates.}
        \end{table}

    \newpage
    \bibliographystyle{IEEEtran}
    \bibliography{proposal_refs}
\end{document}
