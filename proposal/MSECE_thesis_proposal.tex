\documentclass[10pt]{article}

\usepackage{cite}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{ragged2e}
\usepackage{newtxtext}
\usepackage{setspace}
\usepackage{sectsty}
\usepackage[numbib]{tocbibind}
\usepackage[lmargin=1.5in,rmargin=1.5in,tmargin=1in]{geometry}

% Conforming somewhat to the NeurIPS format
\settowidth{\parindent}{}
\setlength{\parskip}{5.5pt}
\sectionfont{\large}
\subsectionfont{\normalsize}

\begin{document}
    % Manually specify/format title, because making small formatting changes to the title is too damn frustrating
    \begin{center}
        \LARGE
        %\textbf{Transformer-based Language Modeling of Aerospace Communications}
        %\textbf{Transformer-based Language Modeling of Air-Traffic Communications}
        %\textbf{Transformer-based Language Modeling of Aviation English}
        \textbf{Transformer-based Language Modeling of Aeronautical English}\\
        \Large
        Master of Science in Electrical \& Computer Engineering\\
        Master's Thesis Proposal\\

        \vspace*{0.25in}

        \normalsize
        \textbf{Aaron Van De Brook---2456908}\\
        \textit{Department of Electrical Engineering \& Computer Science}\\
        \textit{Embry-Riddle Aeronautical University}\\
        vandebra@my.erau.edu
    \end{center}

    % TODO: need to write abstract (probably last step)
    %
    % An abstract. In 100 to 350 words, briefly summarize the problem to solve, the objectives of the project,
    % and the methodology to use.
    \begin{center}
        \section*{\textit{Abstract}}
        \noindent
        \justifying
        \textit{
            \lipsum[1]
        }
    \end{center}

    % An introduction (to the problems to be addressed). In a few paragraphs, clearly define what the problem is being
    % addressed/solved in the thesis. Background information is needed. A literature review is needed to provide an
    % account of the state-of-the-art solutions of this problem by other researchers or industry experts.
    % Objectives of the thesis should be provided here as well.
    \section{Introduction}
        The Transformer neural network architecture was originally designed and created for Neural Machine
        Translation (NMT) tasks \cite{vaswani_attention_2017} and has since seen massive success in various
        other Natural Language Processing (NLP) tasks such as language modeling (both casual and masked language
        modeling tasks) \cite{devlin_bert_2019}, prompt completion \cite{radford_improving_2018}, and
        sequence-to-sequence (S2S) classification \cite{lewis_bart_2019}.
        Recently, Automatic Speech Recognition (ASR) models have started to incorporate language models
        \cite{badrinath_automatic_2022} (sometimes also referred to as linguistic models) in addition to
        acoustic models \cite{li_jasper_2019} as the state of the art models begin to shift towards generative
        end-to-end approaches \cite{hannun_deep_2014}.
        At the time of writing this, the current state of the art ASR models are a mix between Transformer
        and Convolutional neural network (CNN) architectures
        \footnote{According to \url{https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean}}
        \cite{baevski_wav2vec_2020}.


        As ASR models begin to breakout into the aerospace domain, one of the most limiting factors in their
        development and deployment has been their relatively high word error rate(s) (WER)
        \cite{smidl_air_2019,zuluaga-gomez_automatic_2020,badrinath_automatic_2022} as compared to typical state of the art models.
        This is due, at least in part, to the severe lack of transcribed data as compared to the more general ASR domain.
        For instance the best estimates of combined labeled datasets for Air Traffic Control (ATC)
        communications reach approximately 180 hours of speech data \cite{zuluaga-gomez_automatic_2020} whereas
        state of the art ASR datasets typically reach approximately 1000 hours of speech data \cite{panayotov_librispeech_2015}.
        This has lead to the recommendation of semi-supervised learning models wherein unlabeled air
        traffic communications are transcribed by a pretrained ASR model with a relatively low WER.
        The transcriptions are subsequently scored by a language model to obtain some insight into the
        transcription quality \cite{badrinath_automatic_2022,zuluaga-gomez_contextual_2021}.
        Depending on the distribution of the transcription scores, a certain percentage (best $n$ candidates)
        of the transcribed data is used to further fine-tune the ASR model.


        Despite the effectiveness of transformer-based architectures for language modeling NLP tasks
        \cite{devlin_bert_2019,lewis_bart_2019,liu_roberta_2019}, there is little to no implementation
        of them for modeling ATC communications for semi-supervised learning or otherwise in the
        existing literature.
        Additionally, my preliminary work into the transfer learning of BERT models in the ATC
        domain yield surprisingly high perplexity scores before overfitting to the training data.
        This suggests that a more sophisticated approach is needed to make use of transformer-based
        architectures such as BERT or RoBERTa for modeling air traffic communications.

    % TODO: formal literature review
    \section{Background \& Literature}
        % transformer history
        The transformer neural network architecture was proposed in 2017 for Neural Machine Translation tasks and immediately achieved
        state-of-the-art (28.4 BLEU on WMT 2014 English-to-German; 41.8 BLEU on WMT English-to-French datasets)
        \cite{vaswani_attention_2017}. Transformer architectures have been found to be extremely effective at learning representations
        and understandings of languages to predict token probabilities as opposed to transforming one language into another
        \cite{devlin_bert_2019,liu_roberta_2019}. Transformers have also been found to be very effective at
        other NLP-related tasks such as prompt completion and sentiment analysis among others (i.e.~auto-regressive and sequence
        classification tasks, respectively) \cite{lewis_bart_2019,radford_improving_2018}.


        % ASR context (suggest some need for semi-supervised approaches and therefore LMs)
        End-to-end generative models for automatic speech recognition models have made significant progress in recent years with
        current state-of-the-art models\footnote{Current at time of writing} achieving WERs as low as 2\% on LibriSpeech test sets
        \cite{han_contextnet_2020,kriman_quartznet_2020,baevski_wav2vec_2020,li_jasper_2019}. This has led to the development of ASR
        models for the aeronautical domain, specifically, in air traffic control communications
        \cite{badrinath_automatic_2022,smidl_air_2019,zuluaga-gomez_automatic_2020,srinivasamurthy_semi-supervised_2017}.
        However, due to the lack of transcribed data in the aeronautical domain
        \cite{zuluaga-gomez_automatic_2020,srinivasamurthy_semi-supervised_2017,badrinath_automatic_2022,smidl_air_2019}
        ASR models maintain relatively high WERs compared to their counterparts in the more generalized ASR domain
        \cite{zuluaga-gomez_automatic_2020,badrinath_automatic_2022}. Transfer learning has even yielded limited results
        in this domain (depending on model architecture and dataset quality)
        \cite{badrinath_automatic_2022,zuluaga-gomez_automatic_2020}.


        % Unsupervised/semi-supervised in general and LMs commonly used with them
        Unsupervised and semi-supervised methodologies have become popular recently in attempts to address limited data availability
        and develop new approaches towards modeling human speech (notably, wav2vec has achieved state-of-the-art performance with
        very little training data)
        \cite{baevski_wav2vec_2020,badrinath_automatic_2022,srinivasamurthy_semi-supervised_2017,zuluaga-gomez_contextual_2021}.
        Language models are an integral part semi-supervised learning. They are used to obtain a certainty score (or uncertainty
        score, as the case may be) for the predicted text, these are usually either word lattices or N-gram models
        \cite{badrinath_automatic_2022,srinivasamurthy_semi-supervised_2017,zuluaga-gomez_contextual_2021}.
        While these have proven to be good enough for most self-supervised learning tasks, they are hardly state-of-the-art.
        Wav2vec, the current state-of-the-art unsupervised model (and possibly top performing overall), uses a contrastive process
        between convolutional feature extraction and transformer context representations \cite{baevski_wav2vec_2020}.


        % LMs and NLP in aero. domain


    % Methodology. Describe the approaches you plan to use for solving the problem at hand. Specify the
    % tools/instruments/procedures/designs you need to use to finish your work. Also specify other resources,
    % including the budgets, you need to use to finish your work.
    \section{Methodology}
        \subsection{Environment and Source Code Management}
            For ease of use and compatibility with existing libraries, Python (version 3.8 or later)
            will be used to build, train, and test the models as well as to perform data analysis
            where necessary.
            Package/environment management will be done with Anaconda and source code management
            will be done with Git (repository will be publicly accessible via GitHub).

        \subsection{Model Development}
            Fortunately, there are a plethora of libraries available for developing machine learning,
            deep learning, and NLP models.
            The main packages that have been chosen (listed in Appendix A) have front-end language
            bindings for Python and most have back-end APIs written in C/C++ and Rust for streamlined computations.

        % TODO: more work and reading into standard model metrics for LMs
        \subsection{Model Evaluation Metrics}

        % TODO: review min and max hardware required to train models i.e. bare minimum to train model,
        % preferences for efficient training (workstation with GPU/TPU, AWS/Azure compute cluster, etc.)
        \subsection{Resource Requirements}

    % Deliverables and timeline. Provide a detailed schedule and deliverables, shown in a table like the one below.
    \section{Deliverables \& Timeline}
        \begin{table}[h!]
            \centering
            \begin{tabular}{|c|c|c|}
                \hline
                \# & Deliverable & Date\\
                \hline
                0 & Submit thesis proposal & \today\\
                1 & & \\
                \hline
            \end{tabular}
            \label{table:timeline}
            \caption{Deliverables and approximate delivery dates.}
        \end{table}

    \newpage
    \section{Appendix A}
        Packages for Language Model Development:
        \begin{multicols}{2}
            \begin{itemize}
                \item \href{https://pytorch.org}{PyTorch}
                \item \href{https://www.pytorchlightning.ai}{PyTorch Lightning}
                \item \href{https://github.com/huggingface/transformers}{HuggingFace's Transformers}
                \item \href{https://github.com/huggingface/tokenizers}{HuggingFace's Tokenizers}
                \item \href{https://github.com/google/sentencepiece}{Google's SentencePiece}
                \item \href{https://www.nltk.org}{Natural Lanuage Toolkit}
                \item \href{https://scikit-learn.org}{Sci-Kit Learn}
            \end{itemize}
        \end{multicols}

        Packages for Data Analysis:
        \begin{itemize}
            \item \href{https://numpy.org}{NumPy}
            \item \href{https://pandas.pydata.org}{Pandas}
            \item \href{https://matplotlib.org}{Matplotlib}
            \item \href{https://scikit-learn.org}{Sci-Kit Learn}
            \item \href{https://seaborn.pydata.org}{Seaborn}
        \end{itemize}

    \newpage
    % References. List the references in IEEE format.
    \bibliographystyle{IEEEtran}
    \bibliography{proposal_refs}
\end{document}
