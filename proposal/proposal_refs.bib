
@misc{kudo_subword_2018,
	title = {Subword {Regularization}: {Improving} {Neural} {Network} {Translation} {Models} with {Multiple} {Subword} {Candidates}},
	shorttitle = {Subword {Regularization}},
	url = {http://arxiv.org/abs/1804.10959},
	doi = {10.48550/arXiv.1804.10959},
	abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
	urldate = {2022-07-13},
	publisher = {arXiv},
	author = {Kudo, Taku},
	month = apr,
	year = {2018},
	note = {arXiv:1804.10959 [cs]},
	keywords = {Computer Science - Computation and Language, Read},
	file = {arXiv Fulltext PDF:C\:\\Users\\aaron\\Zotero\\storage\\KR7VZXRP\\Kudo - 2018 - Subword Regularization Improving Neural Network T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aaron\\Zotero\\storage\\D5I5TK6S\\1804.html:text/html},
}

@misc{bostrom_byte_2020,
	title = {Byte {Pair} {Encoding} is {Suboptimal} for {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/2004.03720},
	doi = {10.48550/arXiv.2004.03720},
	abstract = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.},
	urldate = {2022-07-13},
	publisher = {arXiv},
	author = {Bostrom, Kaj and Durrett, Greg},
	month = oct,
	year = {2020},
	note = {arXiv:2004.03720 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7, Read},
	file = {arXiv Fulltext PDF:C\:\\Users\\aaron\\Zotero\\storage\\XNYLFZHQ\\Bostrom and Durrett - 2020 - Byte Pair Encoding is Suboptimal for Language Mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aaron\\Zotero\\storage\\YJ64W2P5\\2004.html:text/html},
}

@article{badrinath_automatic_2022,
	title = {Automatic {Speech} {Recognition} for {Air} {Traffic} {Control} {Communications}},
	volume = {2676},
	issn = {0361-1981},
	url = {https://doi.org/10.1177/03611981211036359},
	doi = {10.1177/03611981211036359},
	abstract = {A significant fraction of communications between air traffic controllers and pilots is through speech, via radio channels. Automatic transcription of air traffic control (ATC) communications has the potential to improve system safety, operational performance, and conformance monitoring, and to enhance air traffic controller training. We present an automatic speech recognition model tailored to the ATC domain that can transcribe ATC voice to text. The transcribed text is used to extract operational information such as call-sign and runway number. The models are based on recent improvements in machine learning techniques for speech recognition and natural language processing. We evaluate the performance of the model on diverse datasets.},
	language = {en},
	number = {1},
	urldate = {2022-07-13},
	journal = {Transportation Research Record},
	author = {Badrinath, Sandeep and Balakrishnan, Hamsa},
	month = jan,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	keywords = {Read},
	pages = {798--810},
	file = {SAGE PDF Full Text:C\:\\Users\\aaron\\Zotero\\storage\\D5PKXU75\\Badrinath and Balakrishnan - 2022 - Automatic Speech Recognition for Air Traffic Contr.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-08-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\aaron\\Zotero\\storage\\JQ74ZTK9\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aaron\\Zotero\\storage\\VR3VMCGD\\1810.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {https://arxiv.org/abs/1706.03762v5},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2022-08-21},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\aaron\\Zotero\\storage\\49LY9A7C\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	note = {Publisher: OpenAI},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\aaron\\Zotero\\storage\\INAUVP7F\\Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aaron\\Zotero\\storage\\VW3BLE6N\\1910.html:text/html},
}

@misc{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\aaron\\Zotero\\storage\\WEU2PI57\\Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\aaron\\Zotero\\storage\\C5PZCI4Q\\1910.html:text/html},
}

@article{li2019jasper,
  title={Jasper: An end-to-end convolutional neural acoustic model},
  author={Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan M and Nguyen, Huyen and Gadde, Ravi Teja},
  journal={arXiv preprint arXiv:1904.03288},
  year={2019}
}

@article{hannun2014deep,
  title={Deep speech: Scaling up end-to-end speech recognition},
  author={Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal={arXiv preprint arXiv:1412.5567},
  year={2014}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@techreport{juan2020automatic,
  title={Automatic speech recognition benchmark for air-traffic communications},
  author={Juan, Zuluaga-Gomez and Motlicek, Petr and Zhan, Qingran and Braun, Rudolf and Vesely, Karel},
  year={2020},
  institution={ISCA}
}

@article{vsmidl2019air,
  title={Air traffic control communication (ATCC) speech corpora and their use for ASR and TTS development},
  author={{\v{S}}m{\'\i}dl, Lubo{\v{s}} and {\v{S}}vec, Jan and Tihelka, Daniel and Matou{\v{s}}ek, Jind{\v{r}}ich and Romportl, Jan and Ircing, Pavel},
  journal={Language Resources and Evaluation},
  volume={53},
  number={3},
  pages={449--464},
  year={2019},
  publisher={Springer}
}

@INPROCEEDINGS{librispeech_2015,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={Librispeech: An ASR corpus based on public domain audio books},
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  doi={10.1109/ICASSP.2015.7178964}
}

@article{zuluaga2021contextual,
  title={Contextual semi-supervised learning: An approach to leverage air-surveillance and untranscribed ATC data in ASR systems},
  author={Zuluaga-Gomez, Juan and Nigmatulina, Iuliia and Prasad, Amrutha and Motlicek, Petr and Vesel{\`y}, Karel and Kocour, Martin and Sz{\"o}ke, Igor},
  journal={arXiv preprint arXiv:2104.03643},
  year={2021}
}

@techreport{srinivasamurthy2017semi,
  title={Semi-supervised learning with semantic knowledge extraction for improved speech recognition in air traffic control},
  author={Srinivasamurthy, Ajay and Motlicek, Petr and Himawan, Ivan and Szaszak, Gyorgy and Oualil, Youssef and Helmke, Hartmut},
  year={2017}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
